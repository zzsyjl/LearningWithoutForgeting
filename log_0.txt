------------------------------ Task 1 Training Begin ------------------------------

The number of train set: 5000. The number of test set: 1000

[  1 / 250], 	Loss: 2.1338
[  2 / 250], 	Loss: 1.8376
[  3 / 250], 	Loss: 1.6965
[  4 / 250], 	Loss: 1.6140
[  5 / 250], 	Loss: 1.5508
[  6 / 250], 	Loss: 1.4832
[  7 / 250], 	Loss: 1.3989
[  8 / 250], 	Loss: 1.3114
[  9 / 250], 	Loss: 1.2747
[ 10 / 250], 	Loss: 1.2253
[ 11 / 250], 	Loss: 1.1975
[ 12 / 250], 	Loss: 1.1526
[ 13 / 250], 	Loss: 1.1409
[ 14 / 250], 	Loss: 1.0446
[ 15 / 250], 	Loss: 1.0075
[ 16 / 250], 	Loss: 1.0061
[ 17 / 250], 	Loss: 0.9333
[ 18 / 250], 	Loss: 0.9363
[ 19 / 250], 	Loss: 0.8891
[ 20 / 250], 	Loss: 0.8401
[ 21 / 250], 	Loss: 0.8336
[ 22 / 250], 	Loss: 0.8144
[ 23 / 250], 	Loss: 0.8633
[ 24 / 250], 	Loss: 0.8020
[ 25 / 250], 	Loss: 0.7674
[ 26 / 250], 	Loss: 0.7225
[ 27 / 250], 	Loss: 0.7266
[ 28 / 250], 	Loss: 0.7067
[ 29 / 250], 	Loss: 0.6521
[ 30 / 250], 	Loss: 0.6592
[ 31 / 250], 	Loss: 0.6361
[ 32 / 250], 	Loss: 0.6191
[ 33 / 250], 	Loss: 0.5858
[ 34 / 250], 	Loss: 0.5904
[ 35 / 250], 	Loss: 0.6223
[ 36 / 250], 	Loss: 0.5413
[ 37 / 250], 	Loss: 0.5071
[ 38 / 250], 	Loss: 0.5166
[ 39 / 250], 	Loss: 0.4991
[ 40 / 250], 	Loss: 0.5124
[ 41 / 250], 	Loss: 0.4700
[ 42 / 250], 	Loss: 0.4439
[ 43 / 250], 	Loss: 0.4439
[ 44 / 250], 	Loss: 0.4329
[ 45 / 250], 	Loss: 0.4132
[ 46 / 250], 	Loss: 0.4186
[ 47 / 250], 	Loss: 0.3419
[ 48 / 250], 	Loss: 0.3614
[ 49 / 250], 	Loss: 0.3911
[ 50 / 250], 	Loss: 0.3622
[ 51 / 250], 	Loss: 0.3740
[ 52 / 250], 	Loss: 0.3761
[ 53 / 250], 	Loss: 0.4060
[ 54 / 250], 	Loss: 0.3037
[ 55 / 250], 	Loss: 0.3352
[ 56 / 250], 	Loss: 0.3459
[ 57 / 250], 	Loss: 0.3021
[ 58 / 250], 	Loss: 0.3386
[ 59 / 250], 	Loss: 0.2922
[ 60 / 250], 	Loss: 0.2715
[ 61 / 250], 	Loss: 0.2767
[ 62 / 250], 	Loss: 0.2589
[ 63 / 250], 	Loss: 0.2402
[ 64 / 250], 	Loss: 0.2189
[ 65 / 250], 	Loss: 0.2268
[ 66 / 250], 	Loss: 0.2004
[ 67 / 250], 	Loss: 0.2514
[ 68 / 250], 	Loss: 0.2536
[ 69 / 250], 	Loss: 0.2356
[ 70 / 250], 	Loss: 0.1888
[ 71 / 250], 	Loss: 0.2320
[ 72 / 250], 	Loss: 0.2062
[ 73 / 250], 	Loss: 0.1699
[ 74 / 250], 	Loss: 0.2442
[ 75 / 250], 	Loss: 0.1875
[ 76 / 250], 	Loss: 0.1561
[ 77 / 250], 	Loss: 0.2126
[ 78 / 250], 	Loss: 0.2420
[ 79 / 250], 	Loss: 0.1498
[ 80 / 250], 	Loss: 0.1227
[ 81 / 250], 	Loss: 0.1121
[ 82 / 250], 	Loss: 0.2221
[ 83 / 250], 	Loss: 0.1570
[ 84 / 250], 	Loss: 0.1326
[ 85 / 250], 	Loss: 0.1526
[ 86 / 250], 	Loss: 0.1665
[ 87 / 250], 	Loss: 0.1507
[ 88 / 250], 	Loss: 0.1070
[ 89 / 250], 	Loss: 0.0860
[ 90 / 250], 	Loss: 0.1393
[ 91 / 250], 	Loss: 0.1116
[ 92 / 250], 	Loss: 0.1193
[ 93 / 250], 	Loss: 0.1498
[ 94 / 250], 	Loss: 0.1095
[ 95 / 250], 	Loss: 0.1057
[ 96 / 250], 	Loss: 0.2661
[ 97 / 250], 	Loss: 0.1167
[ 98 / 250], 	Loss: 0.1078
[ 99 / 250], 	Loss: 0.1317
[100 / 250], 	Loss: 0.1143
[101 / 250], 	Loss: 0.0585
[102 / 250], 	Loss: 0.0395
[103 / 250], 	Loss: 0.0382
[104 / 250], 	Loss: 0.0358
[105 / 250], 	Loss: 0.0392
[106 / 250], 	Loss: 0.0337
[107 / 250], 	Loss: 0.0248
[108 / 250], 	Loss: 0.0300
[109 / 250], 	Loss: 0.0231
[110 / 250], 	Loss: 0.0248
[111 / 250], 	Loss: 0.0567
[112 / 250], 	Loss: 0.0267
[113 / 250], 	Loss: 0.0206
[114 / 250], 	Loss: 0.0217
[115 / 250], 	Loss: 0.0218
[116 / 250], 	Loss: 0.0208
[117 / 250], 	Loss: 0.0183
[118 / 250], 	Loss: 0.0159
[119 / 250], 	Loss: 0.0176
[120 / 250], 	Loss: 0.0170
[121 / 250], 	Loss: 0.0122
[122 / 250], 	Loss: 0.0169
[123 / 250], 	Loss: 0.0172
[124 / 250], 	Loss: 0.0179
[125 / 250], 	Loss: 0.0209
[126 / 250], 	Loss: 0.0190
[127 / 250], 	Loss: 0.0280
[128 / 250], 	Loss: 0.0212
[129 / 250], 	Loss: 0.0241
[130 / 250], 	Loss: 0.0147
[131 / 250], 	Loss: 0.0117
[132 / 250], 	Loss: 0.0164
[133 / 250], 	Loss: 0.0149
[134 / 250], 	Loss: 0.0156
[135 / 250], 	Loss: 0.0130
[136 / 250], 	Loss: 0.0109
[137 / 250], 	Loss: 0.0107
[138 / 250], 	Loss: 0.0146
[139 / 250], 	Loss: 0.0145
[140 / 250], 	Loss: 0.0215
[141 / 250], 	Loss: 0.0153
[142 / 250], 	Loss: 0.0129
[143 / 250], 	Loss: 0.0117
[144 / 250], 	Loss: 0.0115
[145 / 250], 	Loss: 0.0083
[146 / 250], 	Loss: 0.0102
[147 / 250], 	Loss: 0.0204
[148 / 250], 	Loss: 0.0168
[149 / 250], 	Loss: 0.0143
[150 / 250], 	Loss: 0.0118
[151 / 250], 	Loss: 0.0104
[152 / 250], 	Loss: 0.0119
[153 / 250], 	Loss: 0.0076
[154 / 250], 	Loss: 0.0089
[155 / 250], 	Loss: 0.0177
[156 / 250], 	Loss: 0.0095
[157 / 250], 	Loss: 0.0203
[158 / 250], 	Loss: 0.0083
[159 / 250], 	Loss: 0.0104
[160 / 250], 	Loss: 0.0202
[161 / 250], 	Loss: 0.0090
[162 / 250], 	Loss: 0.0084
[163 / 250], 	Loss: 0.0082
[164 / 250], 	Loss: 0.0089
[165 / 250], 	Loss: 0.0157
[166 / 250], 	Loss: 0.0137
[167 / 250], 	Loss: 0.0104
[168 / 250], 	Loss: 0.0088
[169 / 250], 	Loss: 0.0095
[170 / 250], 	Loss: 0.0093
[171 / 250], 	Loss: 0.0117
[172 / 250], 	Loss: 0.0124
[173 / 250], 	Loss: 0.0098
[174 / 250], 	Loss: 0.0168
[175 / 250], 	Loss: 0.0082
[176 / 250], 	Loss: 0.0145
[177 / 250], 	Loss: 0.0100
[178 / 250], 	Loss: 0.0089
[179 / 250], 	Loss: 0.0135
[180 / 250], 	Loss: 0.0081
[181 / 250], 	Loss: 0.0096
[182 / 250], 	Loss: 0.0099
[183 / 250], 	Loss: 0.0091
[184 / 250], 	Loss: 0.0096
[185 / 250], 	Loss: 0.0098
[186 / 250], 	Loss: 0.0105
[187 / 250], 	Loss: 0.0078
[188 / 250], 	Loss: 0.0084
[189 / 250], 	Loss: 0.0101
[190 / 250], 	Loss: 0.0117
[191 / 250], 	Loss: 0.0096
[192 / 250], 	Loss: 0.0100
[193 / 250], 	Loss: 0.0095
[194 / 250], 	Loss: 0.0065
[195 / 250], 	Loss: 0.0078
[196 / 250], 	Loss: 0.0114
[197 / 250], 	Loss: 0.0087
[198 / 250], 	Loss: 0.0102
[199 / 250], 	Loss: 0.0109
[200 / 250], 	Loss: 0.0070
[201 / 250], 	Loss: 0.0075
[202 / 250], 	Loss: 0.0089
[203 / 250], 	Loss: 0.0063
[204 / 250], 	Loss: 0.0110
[205 / 250], 	Loss: 0.0098
[206 / 250], 	Loss: 0.0085
[207 / 250], 	Loss: 0.0082
[208 / 250], 	Loss: 0.0125
[209 / 250], 	Loss: 0.0159
[210 / 250], 	Loss: 0.0089
[211 / 250], 	Loss: 0.0082
[212 / 250], 	Loss: 0.0093
[213 / 250], 	Loss: 0.0080
[214 / 250], 	Loss: 0.0094
[215 / 250], 	Loss: 0.0114
[216 / 250], 	Loss: 0.0091
[217 / 250], 	Loss: 0.0093
[218 / 250], 	Loss: 0.0063
[219 / 250], 	Loss: 0.0096
[220 / 250], 	Loss: 0.0099
[221 / 250], 	Loss: 0.0127
[222 / 250], 	Loss: 0.0074
[223 / 250], 	Loss: 0.0077
[224 / 250], 	Loss: 0.0082
[225 / 250], 	Loss: 0.0146
[226 / 250], 	Loss: 0.0107
[227 / 250], 	Loss: 0.0141
[228 / 250], 	Loss: 0.0083
[229 / 250], 	Loss: 0.0071
[230 / 250], 	Loss: 0.0079
[231 / 250], 	Loss: 0.0099
[232 / 250], 	Loss: 0.0133
[233 / 250], 	Loss: 0.0076
[234 / 250], 	Loss: 0.0093
[235 / 250], 	Loss: 0.0127
[236 / 250], 	Loss: 0.0109
[237 / 250], 	Loss: 0.0072
[238 / 250], 	Loss: 0.0101
[239 / 250], 	Loss: 0.0161
[240 / 250], 	Loss: 0.0079
[241 / 250], 	Loss: 0.0259
[242 / 250], 	Loss: 0.0094
[243 / 250], 	Loss: 0.0089
[244 / 250], 	Loss: 0.0106
[245 / 250], 	Loss: 0.0082
[246 / 250], 	Loss: 0.0085
[247 / 250], 	Loss: 0.0177
[248 / 250], 	Loss: 0.0149
[249 / 250], 	Loss: 0.0074
[250 / 250], 	Loss: 0.0083

The accuracy of the 1st task is: 80.5000%
The accuracy of current task 1 is: 80.5000%
The average accuracy is: 80.5000%

------------------------------ Task 1 Training End ------------------------------


------------------------------ Task 2 Training Begin ------------------------------

The number of train set: 5000. The number of test set: 1000

# new classes:  10
[  1 / 250], 	Loss: 2.5914
[  2 / 250], 	Loss: 2.1939
[  3 / 250], 	Loss: 2.0289
[  4 / 250], 	Loss: 1.9662
[  5 / 250], 	Loss: 1.8628
[  6 / 250], 	Loss: 1.8492
[  7 / 250], 	Loss: 1.7429
[  8 / 250], 	Loss: 1.7629
[  9 / 250], 	Loss: 1.6857
[ 10 / 250], 	Loss: 1.6459
[ 11 / 250], 	Loss: 1.6099
[ 12 / 250], 	Loss: 1.5806
[ 13 / 250], 	Loss: 1.5682
[ 14 / 250], 	Loss: 1.5539
[ 15 / 250], 	Loss: 1.5205
[ 16 / 250], 	Loss: 1.4942
[ 17 / 250], 	Loss: 1.5022
[ 18 / 250], 	Loss: 1.4993
[ 19 / 250], 	Loss: 1.4788
[ 20 / 250], 	Loss: 1.4211
[ 21 / 250], 	Loss: 1.4035
[ 22 / 250], 	Loss: 1.4142
[ 23 / 250], 	Loss: 1.3685
[ 24 / 250], 	Loss: 1.4171
[ 25 / 250], 	Loss: 1.3362
[ 26 / 250], 	Loss: 1.3489
[ 27 / 250], 	Loss: 1.3828
[ 28 / 250], 	Loss: 1.3308
[ 29 / 250], 	Loss: 1.3041
[ 30 / 250], 	Loss: 1.2925
[ 31 / 250], 	Loss: 1.3350
[ 32 / 250], 	Loss: 1.2340
[ 33 / 250], 	Loss: 1.2425
[ 34 / 250], 	Loss: 1.2569
[ 35 / 250], 	Loss: 1.2470
[ 36 / 250], 	Loss: 1.2322
[ 37 / 250], 	Loss: 1.2249
[ 38 / 250], 	Loss: 1.2801
[ 39 / 250], 	Loss: 1.2264
[ 40 / 250], 	Loss: 1.4248
[ 41 / 250], 	Loss: 1.2899
[ 42 / 250], 	Loss: 1.1916
[ 43 / 250], 	Loss: 1.1906
[ 44 / 250], 	Loss: 1.2985
[ 45 / 250], 	Loss: 1.3063
[ 46 / 250], 	Loss: 1.2152
[ 47 / 250], 	Loss: 1.1941
[ 48 / 250], 	Loss: 1.2226
[ 49 / 250], 	Loss: 1.3188
[ 50 / 250], 	Loss: 1.2302
[ 51 / 250], 	Loss: 1.1427
[ 52 / 250], 	Loss: 1.1157
[ 53 / 250], 	Loss: 1.1277
[ 54 / 250], 	Loss: 1.1188
[ 55 / 250], 	Loss: 1.1482
[ 56 / 250], 	Loss: 1.3420
[ 57 / 250], 	Loss: 1.1963
[ 58 / 250], 	Loss: 1.1907
[ 59 / 250], 	Loss: 1.1154
[ 60 / 250], 	Loss: 1.0998
[ 61 / 250], 	Loss: 1.1087
[ 62 / 250], 	Loss: 1.2508
[ 63 / 250], 	Loss: 1.2642
[ 64 / 250], 	Loss: 1.1159
[ 65 / 250], 	Loss: 1.1750
[ 66 / 250], 	Loss: 1.1219
[ 67 / 250], 	Loss: 1.1633
[ 68 / 250], 	Loss: 1.2137
[ 69 / 250], 	Loss: 1.1513
[ 70 / 250], 	Loss: 1.1072
[ 71 / 250], 	Loss: 1.1968
[ 72 / 250], 	Loss: 1.1481
[ 73 / 250], 	Loss: 1.0795
[ 74 / 250], 	Loss: 1.0926
[ 75 / 250], 	Loss: 1.0610
[ 76 / 250], 	Loss: 1.0862
[ 77 / 250], 	Loss: 1.0793
[ 78 / 250], 	Loss: 1.2060
[ 79 / 250], 	Loss: 1.2601
[ 80 / 250], 	Loss: 1.1392
[ 81 / 250], 	Loss: 1.2423
[ 82 / 250], 	Loss: 1.1158
[ 83 / 250], 	Loss: 1.1564
[ 84 / 250], 	Loss: 1.2249
[ 85 / 250], 	Loss: 1.0900
[ 86 / 250], 	Loss: 1.0670
[ 87 / 250], 	Loss: 1.0667
[ 88 / 250], 	Loss: 1.0473
[ 89 / 250], 	Loss: 1.0444
[ 90 / 250], 	Loss: 1.0463
[ 91 / 250], 	Loss: 1.0567
[ 92 / 250], 	Loss: 1.0497
[ 93 / 250], 	Loss: 1.0911
[ 94 / 250], 	Loss: 1.0488
[ 95 / 250], 	Loss: 1.0769
[ 96 / 250], 	Loss: 1.1813
[ 97 / 250], 	Loss: 1.1233
[ 98 / 250], 	Loss: 1.0597
[ 99 / 250], 	Loss: 1.0624
[100 / 250], 	Loss: 1.0489
[101 / 250], 	Loss: 1.0172
[102 / 250], 	Loss: 0.9993
[103 / 250], 	Loss: 1.0036
[104 / 250], 	Loss: 0.9921
[105 / 250], 	Loss: 0.9788
[106 / 250], 	Loss: 0.9904
[107 / 250], 	Loss: 0.9907
[108 / 250], 	Loss: 1.0146
[109 / 250], 	Loss: 0.9964
[110 / 250], 	Loss: 0.9911
[111 / 250], 	Loss: 0.9752
[112 / 250], 	Loss: 0.9954
[113 / 250], 	Loss: 0.9822
[114 / 250], 	Loss: 0.9828
[115 / 250], 	Loss: 0.9879
[116 / 250], 	Loss: 0.9903
[117 / 250], 	Loss: 0.9720
[118 / 250], 	Loss: 0.9894
[119 / 250], 	Loss: 0.9763
[120 / 250], 	Loss: 0.9854
[121 / 250], 	Loss: 0.9823
[122 / 250], 	Loss: 0.9775
[123 / 250], 	Loss: 0.9792
[124 / 250], 	Loss: 0.9707
[125 / 250], 	Loss: 0.9948
[126 / 250], 	Loss: 0.9695
[127 / 250], 	Loss: 0.9755
[128 / 250], 	Loss: 0.9708
[129 / 250], 	Loss: 0.9660
[130 / 250], 	Loss: 0.9860
[131 / 250], 	Loss: 0.9753
[132 / 250], 	Loss: 0.9636
[133 / 250], 	Loss: 0.9738
[134 / 250], 	Loss: 0.9661
[135 / 250], 	Loss: 0.9669
[136 / 250], 	Loss: 0.9615
[137 / 250], 	Loss: 0.9680
[138 / 250], 	Loss: 0.9644
[139 / 250], 	Loss: 0.9892
[140 / 250], 	Loss: 0.9790
[141 / 250], 	Loss: 0.9732
[142 / 250], 	Loss: 0.9756
[143 / 250], 	Loss: 0.9709
[144 / 250], 	Loss: 0.9712
[145 / 250], 	Loss: 0.9730
[146 / 250], 	Loss: 0.9727
[147 / 250], 	Loss: 0.9782
[148 / 250], 	Loss: 0.9721
[149 / 250], 	Loss: 0.9735
[150 / 250], 	Loss: 0.9707
[151 / 250], 	Loss: 0.9736
[152 / 250], 	Loss: 0.9685
[153 / 250], 	Loss: 0.9637
[154 / 250], 	Loss: 0.9687
[155 / 250], 	Loss: 0.9674
[156 / 250], 	Loss: 0.9842
[157 / 250], 	Loss: 0.9665
[158 / 250], 	Loss: 0.9672
[159 / 250], 	Loss: 0.9656
[160 / 250], 	Loss: 0.9607
[161 / 250], 	Loss: 0.9713
[162 / 250], 	Loss: 0.9662
[163 / 250], 	Loss: 0.9712
[164 / 250], 	Loss: 0.9585
[165 / 250], 	Loss: 0.9602
[166 / 250], 	Loss: 0.9669
[167 / 250], 	Loss: 0.9706
[168 / 250], 	Loss: 0.9642
[169 / 250], 	Loss: 0.9667
[170 / 250], 	Loss: 0.9639
[171 / 250], 	Loss: 0.9695
[172 / 250], 	Loss: 0.9669
[173 / 250], 	Loss: 0.9786
[174 / 250], 	Loss: 0.9729
[175 / 250], 	Loss: 0.9694
[176 / 250], 	Loss: 0.9640
[177 / 250], 	Loss: 0.9693
[178 / 250], 	Loss: 0.9764
[179 / 250], 	Loss: 0.9749
[180 / 250], 	Loss: 0.9605
[181 / 250], 	Loss: 0.9621
[182 / 250], 	Loss: 0.9677
[183 / 250], 	Loss: 0.9775
[184 / 250], 	Loss: 0.9652
[185 / 250], 	Loss: 0.9663
[186 / 250], 	Loss: 0.9588
[187 / 250], 	Loss: 0.9743
[188 / 250], 	Loss: 0.9676
[189 / 250], 	Loss: 0.9661
[190 / 250], 	Loss: 0.9642
[191 / 250], 	Loss: 0.9884
[192 / 250], 	Loss: 0.9588
[193 / 250], 	Loss: 0.9662
[194 / 250], 	Loss: 0.9627
[195 / 250], 	Loss: 0.9682
[196 / 250], 	Loss: 0.9690
[197 / 250], 	Loss: 0.9671
[198 / 250], 	Loss: 0.9668
[199 / 250], 	Loss: 0.9626
[200 / 250], 	Loss: 0.9639
[201 / 250], 	Loss: 0.9624
[202 / 250], 	Loss: 0.9626
[203 / 250], 	Loss: 0.9592
[204 / 250], 	Loss: 0.9657
[205 / 250], 	Loss: 0.9628
[206 / 250], 	Loss: 0.9565
[207 / 250], 	Loss: 0.9617
[208 / 250], 	Loss: 0.9669
[209 / 250], 	Loss: 0.9586
[210 / 250], 	Loss: 0.9661
[211 / 250], 	Loss: 0.9581
[212 / 250], 	Loss: 0.9638
[213 / 250], 	Loss: 0.9695
[214 / 250], 	Loss: 0.9537
[215 / 250], 	Loss: 0.9683
[216 / 250], 	Loss: 0.9768
[217 / 250], 	Loss: 0.9593
[218 / 250], 	Loss: 0.9630
[219 / 250], 	Loss: 0.9655
[220 / 250], 	Loss: 0.9621
[221 / 250], 	Loss: 0.9678
[222 / 250], 	Loss: 0.9738
[223 / 250], 	Loss: 0.9710
[224 / 250], 	Loss: 0.9724
[225 / 250], 	Loss: 0.9669
[226 / 250], 	Loss: 0.9717
[227 / 250], 	Loss: 0.9677
[228 / 250], 	Loss: 0.9602
[229 / 250], 	Loss: 0.9616
[230 / 250], 	Loss: 0.9612
[231 / 250], 	Loss: 0.9663
[232 / 250], 	Loss: 0.9618
[233 / 250], 	Loss: 0.9709
[234 / 250], 	Loss: 0.9580
[235 / 250], 	Loss: 0.9726
[236 / 250], 	Loss: 0.9682
[237 / 250], 	Loss: 0.9674
[238 / 250], 	Loss: 0.9693
[239 / 250], 	Loss: 0.9644
[240 / 250], 	Loss: 0.9711
[241 / 250], 	Loss: 0.9626
[242 / 250], 	Loss: 0.9582
[243 / 250], 	Loss: 0.9488
[244 / 250], 	Loss: 0.9638
[245 / 250], 	Loss: 0.9581
[246 / 250], 	Loss: 0.9616
[247 / 250], 	Loss: 0.9668
[248 / 250], 	Loss: 0.9658
[249 / 250], 	Loss: 0.9605
[250 / 250], 	Loss: 0.9612

The accuracy of the 1st task is: 60.3000%
The accuracy of current task 2 is: 73.3000%
The average accuracy is: 66.8000%

------------------------------ Task 2 Training End ------------------------------


------------------------------ Task 3 Training Begin ------------------------------

The number of train set: 5000. The number of test set: 1000

# new classes:  10
[  1 / 250], 	Loss: 2.7246
[  2 / 250], 	Loss: 2.3784
[  3 / 250], 	Loss: 2.2898
[  4 / 250], 	Loss: 2.2297
[  5 / 250], 	Loss: 2.1790
[  6 / 250], 	Loss: 2.1297
[  7 / 250], 	Loss: 2.1012
[  8 / 250], 	Loss: 2.0579
[  9 / 250], 	Loss: 2.0153
[ 10 / 250], 	Loss: 1.9669
[ 11 / 250], 	Loss: 1.9901
[ 12 / 250], 	Loss: 1.9157
[ 13 / 250], 	Loss: 1.9410
[ 14 / 250], 	Loss: 1.9583
[ 15 / 250], 	Loss: 1.8987
[ 16 / 250], 	Loss: 1.8845
[ 17 / 250], 	Loss: 1.8223
[ 18 / 250], 	Loss: 1.8379
[ 19 / 250], 	Loss: 1.8303
[ 20 / 250], 	Loss: 1.7856
[ 21 / 250], 	Loss: 1.7651
[ 22 / 250], 	Loss: 1.7411
[ 23 / 250], 	Loss: 1.7300
[ 24 / 250], 	Loss: 1.7597
[ 25 / 250], 	Loss: 1.7070
[ 26 / 250], 	Loss: 1.6627
[ 27 / 250], 	Loss: 1.7437
[ 28 / 250], 	Loss: 1.6429
[ 29 / 250], 	Loss: 1.6577
[ 30 / 250], 	Loss: 1.7141
[ 31 / 250], 	Loss: 1.6884
[ 32 / 250], 	Loss: 1.6064
[ 33 / 250], 	Loss: 1.6545
[ 34 / 250], 	Loss: 1.6521
[ 35 / 250], 	Loss: 1.7108
[ 36 / 250], 	Loss: 1.5921
[ 37 / 250], 	Loss: 1.6735
[ 38 / 250], 	Loss: 1.6211
[ 39 / 250], 	Loss: 1.6100
[ 40 / 250], 	Loss: 1.7507
[ 41 / 250], 	Loss: 1.5989
[ 42 / 250], 	Loss: 1.5704
[ 43 / 250], 	Loss: 1.6179
[ 44 / 250], 	Loss: 1.5632
[ 45 / 250], 	Loss: 1.5632
[ 46 / 250], 	Loss: 1.5463
[ 47 / 250], 	Loss: 1.5273
[ 48 / 250], 	Loss: 1.5280
[ 49 / 250], 	Loss: 1.5577
[ 50 / 250], 	Loss: 1.6792
[ 51 / 250], 	Loss: 1.6232
[ 52 / 250], 	Loss: 1.6558
[ 53 / 250], 	Loss: 1.5258
[ 54 / 250], 	Loss: 1.5087
[ 55 / 250], 	Loss: 1.5353
[ 56 / 250], 	Loss: 1.5808
[ 57 / 250], 	Loss: 1.5150
[ 58 / 250], 	Loss: 1.5691
[ 59 / 250], 	Loss: 1.5032
[ 60 / 250], 	Loss: 1.5345
[ 61 / 250], 	Loss: 1.4817
[ 62 / 250], 	Loss: 1.4873
[ 63 / 250], 	Loss: 1.4616
[ 64 / 250], 	Loss: 1.5345
[ 65 / 250], 	Loss: 1.5548
[ 66 / 250], 	Loss: 1.5355
[ 67 / 250], 	Loss: 1.4914
[ 68 / 250], 	Loss: 1.4535
[ 69 / 250], 	Loss: 1.4551
[ 70 / 250], 	Loss: 1.5126
[ 71 / 250], 	Loss: 1.5225
[ 72 / 250], 	Loss: 1.4837
[ 73 / 250], 	Loss: 1.4749
[ 74 / 250], 	Loss: 1.5019
[ 75 / 250], 	Loss: 1.5515
[ 76 / 250], 	Loss: 1.5008
[ 77 / 250], 	Loss: 1.5848
[ 78 / 250], 	Loss: 1.5520
[ 79 / 250], 	Loss: 1.4724
[ 80 / 250], 	Loss: 1.4507
[ 81 / 250], 	Loss: 1.5315
[ 82 / 250], 	Loss: 1.5912
[ 83 / 250], 	Loss: 1.5759
[ 84 / 250], 	Loss: 1.5499
[ 85 / 250], 	Loss: 1.5245
[ 86 / 250], 	Loss: 1.4852
[ 87 / 250], 	Loss: 1.4699
[ 88 / 250], 	Loss: 1.4654
[ 89 / 250], 	Loss: 1.4847
[ 90 / 250], 	Loss: 1.4584
[ 91 / 250], 	Loss: 1.4481
[ 92 / 250], 	Loss: 1.4704
[ 93 / 250], 	Loss: 1.5058
[ 94 / 250], 	Loss: 1.4802
[ 95 / 250], 	Loss: 1.6781
[ 96 / 250], 	Loss: 1.5214
[ 97 / 250], 	Loss: 1.5210
[ 98 / 250], 	Loss: 1.4641
[ 99 / 250], 	Loss: 1.4630
[100 / 250], 	Loss: 1.4442
[101 / 250], 	Loss: 1.4266
[102 / 250], 	Loss: 1.3937
[103 / 250], 	Loss: 1.4010
[104 / 250], 	Loss: 1.4158
[105 / 250], 	Loss: 1.4029
[106 / 250], 	Loss: 1.3970
[107 / 250], 	Loss: 1.3851
[108 / 250], 	Loss: 1.3902
[109 / 250], 	Loss: 1.3953
[110 / 250], 	Loss: 1.3783
[111 / 250], 	Loss: 1.3850
[112 / 250], 	Loss: 1.3833
[113 / 250], 	Loss: 1.3822
[114 / 250], 	Loss: 1.4002
[115 / 250], 	Loss: 1.3831
[116 / 250], 	Loss: 1.3935
[117 / 250], 	Loss: 1.3840
[118 / 250], 	Loss: 1.3824
[119 / 250], 	Loss: 1.3869
[120 / 250], 	Loss: 1.3942
[121 / 250], 	Loss: 1.3953
[122 / 250], 	Loss: 1.3774
[123 / 250], 	Loss: 1.3887
[124 / 250], 	Loss: 1.3814
[125 / 250], 	Loss: 1.3893
[126 / 250], 	Loss: 1.3863
[127 / 250], 	Loss: 1.3833
[128 / 250], 	Loss: 1.3874
[129 / 250], 	Loss: 1.3839
[130 / 250], 	Loss: 1.3707
[131 / 250], 	Loss: 1.3750
[132 / 250], 	Loss: 1.3912
[133 / 250], 	Loss: 1.3726
[134 / 250], 	Loss: 1.3880
[135 / 250], 	Loss: 1.3841
[136 / 250], 	Loss: 1.3718
[137 / 250], 	Loss: 1.3787
[138 / 250], 	Loss: 1.3791
[139 / 250], 	Loss: 1.3683
[140 / 250], 	Loss: 1.3853
[141 / 250], 	Loss: 1.3710
[142 / 250], 	Loss: 1.3601
[143 / 250], 	Loss: 1.3711
[144 / 250], 	Loss: 1.3720
[145 / 250], 	Loss: 1.3766
[146 / 250], 	Loss: 1.3781
[147 / 250], 	Loss: 1.4063
[148 / 250], 	Loss: 1.3780
[149 / 250], 	Loss: 1.3818
[150 / 250], 	Loss: 1.3687
[151 / 250], 	Loss: 1.3670
[152 / 250], 	Loss: 1.3576
[153 / 250], 	Loss: 1.3677
[154 / 250], 	Loss: 1.3765
[155 / 250], 	Loss: 1.3844
[156 / 250], 	Loss: 1.3639
[157 / 250], 	Loss: 1.3727
[158 / 250], 	Loss: 1.3588
[159 / 250], 	Loss: 1.3750
[160 / 250], 	Loss: 1.3688
[161 / 250], 	Loss: 1.3594
[162 / 250], 	Loss: 1.3670
[163 / 250], 	Loss: 1.3612
[164 / 250], 	Loss: 1.3610
[165 / 250], 	Loss: 1.3576
[166 / 250], 	Loss: 1.3719
[167 / 250], 	Loss: 1.3666
[168 / 250], 	Loss: 1.3609
[169 / 250], 	Loss: 1.3794
[170 / 250], 	Loss: 1.3607
[171 / 250], 	Loss: 1.3659
[172 / 250], 	Loss: 1.3625
[173 / 250], 	Loss: 1.3691
[174 / 250], 	Loss: 1.3591
[175 / 250], 	Loss: 1.3702
[176 / 250], 	Loss: 1.3573
[177 / 250], 	Loss: 1.3667
[178 / 250], 	Loss: 1.3694
[179 / 250], 	Loss: 1.3611
[180 / 250], 	Loss: 1.3676
[181 / 250], 	Loss: 1.3600
[182 / 250], 	Loss: 1.3793
[183 / 250], 	Loss: 1.3745
[184 / 250], 	Loss: 1.3625
[185 / 250], 	Loss: 1.3614
[186 / 250], 	Loss: 1.3610
[187 / 250], 	Loss: 1.3666
[188 / 250], 	Loss: 1.3686
[189 / 250], 	Loss: 1.3724
[190 / 250], 	Loss: 1.3586
[191 / 250], 	Loss: 1.3703
[192 / 250], 	Loss: 1.3643
[193 / 250], 	Loss: 1.3620
[194 / 250], 	Loss: 1.3669
[195 / 250], 	Loss: 1.3761
[196 / 250], 	Loss: 1.3675
[197 / 250], 	Loss: 1.3722
[198 / 250], 	Loss: 1.3608
[199 / 250], 	Loss: 1.3630
[200 / 250], 	Loss: 1.3669
[201 / 250], 	Loss: 1.3689
[202 / 250], 	Loss: 1.3702
[203 / 250], 	Loss: 1.3651
[204 / 250], 	Loss: 1.3681
[205 / 250], 	Loss: 1.3676
[206 / 250], 	Loss: 1.3684
[207 / 250], 	Loss: 1.3581
[208 / 250], 	Loss: 1.3597
[209 / 250], 	Loss: 1.3698
[210 / 250], 	Loss: 1.3827
[211 / 250], 	Loss: 1.3676
[212 / 250], 	Loss: 1.3735
[213 / 250], 	Loss: 1.3657
[214 / 250], 	Loss: 1.3568
[215 / 250], 	Loss: 1.3626
[216 / 250], 	Loss: 1.3721
[217 / 250], 	Loss: 1.3600
[218 / 250], 	Loss: 1.3574
[219 / 250], 	Loss: 1.3644
[220 / 250], 	Loss: 1.3663
[221 / 250], 	Loss: 1.3644
[222 / 250], 	Loss: 1.3644
[223 / 250], 	Loss: 1.3674
[224 / 250], 	Loss: 1.3728
[225 / 250], 	Loss: 1.3601
[226 / 250], 	Loss: 1.3697
[227 / 250], 	Loss: 1.3615
[228 / 250], 	Loss: 1.3716
[229 / 250], 	Loss: 1.3590
[230 / 250], 	Loss: 1.3694
[231 / 250], 	Loss: 1.3680
[232 / 250], 	Loss: 1.3677
[233 / 250], 	Loss: 1.3673
[234 / 250], 	Loss: 1.3620
[235 / 250], 	Loss: 1.3739
[236 / 250], 	Loss: 1.3603
[237 / 250], 	Loss: 1.3683
[238 / 250], 	Loss: 1.3687
[239 / 250], 	Loss: 1.3609
[240 / 250], 	Loss: 1.3649
[241 / 250], 	Loss: 1.3713
[242 / 250], 	Loss: 1.3728
[243 / 250], 	Loss: 1.3577
[244 / 250], 	Loss: 1.3670
[245 / 250], 	Loss: 1.3716
[246 / 250], 	Loss: 1.3788
[247 / 250], 	Loss: 1.3716
[248 / 250], 	Loss: 1.3722
[249 / 250], 	Loss: 1.3679
[250 / 250], 	Loss: 1.3704

The accuracy of the 1st task is: 30.3000%
The accuracy of current task 3 is: 70.0000%
The average accuracy is: 48.2000%

------------------------------ Task 3 Training End ------------------------------


------------------------------ Task 4 Training Begin ------------------------------

The number of train set: 5000. The number of test set: 1000

# new classes:  10
[  1 / 250], 	Loss: 2.7255
[  2 / 250], 	Loss: 2.2265
[  3 / 250], 	Loss: 2.1520
[  4 / 250], 	Loss: 2.0983
[  5 / 250], 	Loss: 2.0724
[  6 / 250], 	Loss: 1.9934
[  7 / 250], 	Loss: 1.9699
[  8 / 250], 	Loss: 1.9575
[  9 / 250], 	Loss: 1.8998
[ 10 / 250], 	Loss: 1.8741
[ 11 / 250], 	Loss: 1.8346
[ 12 / 250], 	Loss: 1.8401
[ 13 / 250], 	Loss: 1.8071
[ 14 / 250], 	Loss: 1.7889
[ 15 / 250], 	Loss: 1.7800
[ 16 / 250], 	Loss: 1.7476
[ 17 / 250], 	Loss: 1.7609
[ 18 / 250], 	Loss: 1.7628
[ 19 / 250], 	Loss: 1.7336
[ 20 / 250], 	Loss: 1.7287
[ 21 / 250], 	Loss: 1.6786
[ 22 / 250], 	Loss: 1.7315
[ 23 / 250], 	Loss: 1.7625
[ 24 / 250], 	Loss: 1.7533
[ 25 / 250], 	Loss: 1.6957
[ 26 / 250], 	Loss: 1.6623
[ 27 / 250], 	Loss: 1.6339
[ 28 / 250], 	Loss: 1.6320
[ 29 / 250], 	Loss: 1.6323
[ 30 / 250], 	Loss: 1.6786
[ 31 / 250], 	Loss: 1.6585
[ 32 / 250], 	Loss: 1.6781
[ 33 / 250], 	Loss: 1.7125
[ 34 / 250], 	Loss: 1.6537
[ 35 / 250], 	Loss: 1.7373
[ 36 / 250], 	Loss: 1.6902
[ 37 / 250], 	Loss: 1.6949
[ 38 / 250], 	Loss: 1.6665
[ 39 / 250], 	Loss: 1.5906
[ 40 / 250], 	Loss: 1.6115
[ 41 / 250], 	Loss: 1.6063
[ 42 / 250], 	Loss: 1.6107
[ 43 / 250], 	Loss: 1.6046
[ 44 / 250], 	Loss: 1.5863
[ 45 / 250], 	Loss: 1.6156
[ 46 / 250], 	Loss: 1.5946
[ 47 / 250], 	Loss: 1.6183
[ 48 / 250], 	Loss: 1.5879
[ 49 / 250], 	Loss: 1.6466
[ 50 / 250], 	Loss: 1.6272
[ 51 / 250], 	Loss: 1.6139
[ 52 / 250], 	Loss: 1.5851
[ 53 / 250], 	Loss: 1.5758
[ 54 / 250], 	Loss: 1.5759
[ 55 / 250], 	Loss: 1.5597
[ 56 / 250], 	Loss: 1.5601
[ 57 / 250], 	Loss: 1.5701
[ 58 / 250], 	Loss: 1.5624
[ 59 / 250], 	Loss: 1.5418
[ 60 / 250], 	Loss: 1.5634
[ 61 / 250], 	Loss: 1.5842
[ 62 / 250], 	Loss: 1.6586
[ 63 / 250], 	Loss: 1.6392
[ 64 / 250], 	Loss: 1.5743
[ 65 / 250], 	Loss: 1.6569
[ 66 / 250], 	Loss: 1.6123
[ 67 / 250], 	Loss: 1.6037
[ 68 / 250], 	Loss: 1.5738
[ 69 / 250], 	Loss: 1.5597
[ 70 / 250], 	Loss: 1.6010
[ 71 / 250], 	Loss: 1.5677
[ 72 / 250], 	Loss: 1.6527
[ 73 / 250], 	Loss: 1.5785
[ 74 / 250], 	Loss: 1.7124
[ 75 / 250], 	Loss: 1.5753
[ 76 / 250], 	Loss: 1.5519
[ 77 / 250], 	Loss: 1.5861
[ 78 / 250], 	Loss: 1.5683
[ 79 / 250], 	Loss: 1.5476
[ 80 / 250], 	Loss: 1.6389
[ 81 / 250], 	Loss: 1.5729
[ 82 / 250], 	Loss: 1.5761
[ 83 / 250], 	Loss: 1.5551
[ 84 / 250], 	Loss: 1.5794
[ 85 / 250], 	Loss: 1.6057
[ 86 / 250], 	Loss: 1.5778
[ 87 / 250], 	Loss: 1.7486
[ 88 / 250], 	Loss: 1.6418
[ 89 / 250], 	Loss: 1.5826
[ 90 / 250], 	Loss: 1.5813
[ 91 / 250], 	Loss: 1.6288
[ 92 / 250], 	Loss: 1.5684
[ 93 / 250], 	Loss: 1.5578
[ 94 / 250], 	Loss: 1.5476
[ 95 / 250], 	Loss: 1.5372
[ 96 / 250], 	Loss: 1.5914
[ 97 / 250], 	Loss: 1.5313
[ 98 / 250], 	Loss: 1.5189
[ 99 / 250], 	Loss: 1.5771
[100 / 250], 	Loss: 1.5578
[101 / 250], 	Loss: 1.5110
[102 / 250], 	Loss: 1.5191
[103 / 250], 	Loss: 1.5235
[104 / 250], 	Loss: 1.5080
[105 / 250], 	Loss: 1.5026
[106 / 250], 	Loss: 1.5046
[107 / 250], 	Loss: 1.4980
[108 / 250], 	Loss: 1.5014
[109 / 250], 	Loss: 1.5120
[110 / 250], 	Loss: 1.4973
[111 / 250], 	Loss: 1.4976
[112 / 250], 	Loss: 1.5018
[113 / 250], 	Loss: 1.4988
[114 / 250], 	Loss: 1.4992
[115 / 250], 	Loss: 1.4969
[116 / 250], 	Loss: 1.4997
[117 / 250], 	Loss: 1.5020
[118 / 250], 	Loss: 1.5076
[119 / 250], 	Loss: 1.4923
[120 / 250], 	Loss: 1.4891
[121 / 250], 	Loss: 1.4976
[122 / 250], 	Loss: 1.4947
[123 / 250], 	Loss: 1.4920
[124 / 250], 	Loss: 1.4926
[125 / 250], 	Loss: 1.5010
[126 / 250], 	Loss: 1.4894
[127 / 250], 	Loss: 1.4932
[128 / 250], 	Loss: 1.4867
[129 / 250], 	Loss: 1.4939
[130 / 250], 	Loss: 1.4952
[131 / 250], 	Loss: 1.4902
[132 / 250], 	Loss: 1.5007
[133 / 250], 	Loss: 1.4973
[134 / 250], 	Loss: 1.4924
[135 / 250], 	Loss: 1.4868
[136 / 250], 	Loss: 1.4872
[137 / 250], 	Loss: 1.4885
[138 / 250], 	Loss: 1.4947
[139 / 250], 	Loss: 1.4987
[140 / 250], 	Loss: 1.4827
[141 / 250], 	Loss: 1.4913
[142 / 250], 	Loss: 1.4884
[143 / 250], 	Loss: 1.4895
[144 / 250], 	Loss: 1.4806
[145 / 250], 	Loss: 1.4920
[146 / 250], 	Loss: 1.4925
[147 / 250], 	Loss: 1.4893
[148 / 250], 	Loss: 1.4873
[149 / 250], 	Loss: 1.4927
[150 / 250], 	Loss: 1.4883
[151 / 250], 	Loss: 1.4942
[152 / 250], 	Loss: 1.4788
[153 / 250], 	Loss: 1.4924
[154 / 250], 	Loss: 1.4908
[155 / 250], 	Loss: 1.4858
[156 / 250], 	Loss: 1.4834
[157 / 250], 	Loss: 1.4860
[158 / 250], 	Loss: 1.4914
[159 / 250], 	Loss: 1.4901
[160 / 250], 	Loss: 1.4877
[161 / 250], 	Loss: 1.4766
[162 / 250], 	Loss: 1.4754
[163 / 250], 	Loss: 1.4980
[164 / 250], 	Loss: 1.4909
[165 / 250], 	Loss: 1.4858
[166 / 250], 	Loss: 1.4843
[167 / 250], 	Loss: 1.4810
[168 / 250], 	Loss: 1.4728
[169 / 250], 	Loss: 1.4866
[170 / 250], 	Loss: 1.4733
[171 / 250], 	Loss: 1.4943
[172 / 250], 	Loss: 1.4757
[173 / 250], 	Loss: 1.4907
[174 / 250], 	Loss: 1.4915
[175 / 250], 	Loss: 1.4962
[176 / 250], 	Loss: 1.4944
[177 / 250], 	Loss: 1.4974
[178 / 250], 	Loss: 1.4832
[179 / 250], 	Loss: 1.4959
[180 / 250], 	Loss: 1.4755
[181 / 250], 	Loss: 1.4881
[182 / 250], 	Loss: 1.4830
[183 / 250], 	Loss: 1.4895
[184 / 250], 	Loss: 1.4873
[185 / 250], 	Loss: 1.4794
[186 / 250], 	Loss: 1.4921
[187 / 250], 	Loss: 1.4913
[188 / 250], 	Loss: 1.4813
[189 / 250], 	Loss: 1.4888
[190 / 250], 	Loss: 1.5001
[191 / 250], 	Loss: 1.4844
[192 / 250], 	Loss: 1.4842
[193 / 250], 	Loss: 1.4795
[194 / 250], 	Loss: 1.4834
[195 / 250], 	Loss: 1.4810
[196 / 250], 	Loss: 1.4828
[197 / 250], 	Loss: 1.4828
[198 / 250], 	Loss: 1.4848
[199 / 250], 	Loss: 1.4905
[200 / 250], 	Loss: 1.4840
[201 / 250], 	Loss: 1.4882
[202 / 250], 	Loss: 1.4713
[203 / 250], 	Loss: 1.4773
[204 / 250], 	Loss: 1.4845
[205 / 250], 	Loss: 1.4796
[206 / 250], 	Loss: 1.4787
[207 / 250], 	Loss: 1.4784
[208 / 250], 	Loss: 1.4800
[209 / 250], 	Loss: 1.4777
[210 / 250], 	Loss: 1.4852
[211 / 250], 	Loss: 1.4869
[212 / 250], 	Loss: 1.4787
[213 / 250], 	Loss: 1.4848
[214 / 250], 	Loss: 1.4792
[215 / 250], 	Loss: 1.4874
[216 / 250], 	Loss: 1.4818
[217 / 250], 	Loss: 1.4825
[218 / 250], 	Loss: 1.4856
[219 / 250], 	Loss: 1.4887
[220 / 250], 	Loss: 1.4849
[221 / 250], 	Loss: 1.4809
[222 / 250], 	Loss: 1.4890
[223 / 250], 	Loss: 1.4806
[224 / 250], 	Loss: 1.4837
[225 / 250], 	Loss: 1.4798
[226 / 250], 	Loss: 1.4829
[227 / 250], 	Loss: 1.5122
[228 / 250], 	Loss: 1.4720
[229 / 250], 	Loss: 1.4884
[230 / 250], 	Loss: 1.4735
[231 / 250], 	Loss: 1.4815
[232 / 250], 	Loss: 1.4813
[233 / 250], 	Loss: 1.4865
[234 / 250], 	Loss: 1.4814
[235 / 250], 	Loss: 1.4672
[236 / 250], 	Loss: 1.4802
[237 / 250], 	Loss: 1.4809
[238 / 250], 	Loss: 1.4921
[239 / 250], 	Loss: 1.4913
[240 / 250], 	Loss: 1.5007
[241 / 250], 	Loss: 1.4924
[242 / 250], 	Loss: 1.4891
[243 / 250], 	Loss: 1.4739
[244 / 250], 	Loss: 1.4803
[245 / 250], 	Loss: 1.4962
[246 / 250], 	Loss: 1.4835
[247 / 250], 	Loss: 1.4836
[248 / 250], 	Loss: 1.4791
[249 / 250], 	Loss: 1.4877
[250 / 250], 	Loss: 1.4824

The accuracy of the 1st task is: 14.0000%
The accuracy of current task 4 is: 73.0000%
The average accuracy is: 39.1500%

------------------------------ Task 4 Training End ------------------------------


------------------------------ Task 5 Training Begin ------------------------------

The number of train set: 5000. The number of test set: 1000

# new classes:  10
[  1 / 250], 	Loss: 3.1328
[  2 / 250], 	Loss: 2.7398
[  3 / 250], 	Loss: 2.5980
[  4 / 250], 	Loss: 2.5578
[  5 / 250], 	Loss: 2.4742
[  6 / 250], 	Loss: 2.4129
[  7 / 250], 	Loss: 2.3835
[  8 / 250], 	Loss: 2.3141
[  9 / 250], 	Loss: 2.3169
[ 10 / 250], 	Loss: 2.2560
[ 11 / 250], 	Loss: 2.2253
[ 12 / 250], 	Loss: 2.2393
[ 13 / 250], 	Loss: 2.2494
[ 14 / 250], 	Loss: 2.1938
[ 15 / 250], 	Loss: 2.1396
[ 16 / 250], 	Loss: 2.1571
[ 17 / 250], 	Loss: 2.1574
[ 18 / 250], 	Loss: 2.1091
[ 19 / 250], 	Loss: 2.1857
[ 20 / 250], 	Loss: 2.1053
[ 21 / 250], 	Loss: 2.1053
[ 22 / 250], 	Loss: 2.1173
[ 23 / 250], 	Loss: 2.1229
[ 24 / 250], 	Loss: 2.0674
[ 25 / 250], 	Loss: 2.0851
[ 26 / 250], 	Loss: 2.0909
[ 27 / 250], 	Loss: 2.0198
[ 28 / 250], 	Loss: 1.9902
[ 29 / 250], 	Loss: 1.9749
[ 30 / 250], 	Loss: 2.0364
[ 31 / 250], 	Loss: 2.0447
[ 32 / 250], 	Loss: 2.0137
[ 33 / 250], 	Loss: 1.9736
[ 34 / 250], 	Loss: 1.9455
[ 35 / 250], 	Loss: 1.9548
[ 36 / 250], 	Loss: 1.9490
[ 37 / 250], 	Loss: 1.9240
[ 38 / 250], 	Loss: 1.9155
[ 39 / 250], 	Loss: 1.9904
[ 40 / 250], 	Loss: 1.9238
[ 41 / 250], 	Loss: 1.9254
[ 42 / 250], 	Loss: 1.9746
[ 43 / 250], 	Loss: 1.9491
[ 44 / 250], 	Loss: 1.9967
[ 45 / 250], 	Loss: 2.0242
[ 46 / 250], 	Loss: 1.9437
[ 47 / 250], 	Loss: 1.9551
[ 48 / 250], 	Loss: 1.9492
[ 49 / 250], 	Loss: 1.9248
[ 50 / 250], 	Loss: 1.9725
[ 51 / 250], 	Loss: 1.9366
[ 52 / 250], 	Loss: 1.9291
[ 53 / 250], 	Loss: 1.9016
[ 54 / 250], 	Loss: 1.8956
[ 55 / 250], 	Loss: 1.9002
[ 56 / 250], 	Loss: 2.1294
[ 57 / 250], 	Loss: 2.0319
[ 58 / 250], 	Loss: 1.9555
[ 59 / 250], 	Loss: 1.9579
[ 60 / 250], 	Loss: 2.0265
[ 61 / 250], 	Loss: 1.9325
[ 62 / 250], 	Loss: 2.0492
[ 63 / 250], 	Loss: 1.9177
[ 64 / 250], 	Loss: 1.9118
[ 65 / 250], 	Loss: 1.9066
[ 66 / 250], 	Loss: 1.8839
[ 67 / 250], 	Loss: 1.8748
[ 68 / 250], 	Loss: 1.9055
[ 69 / 250], 	Loss: 1.8783
[ 70 / 250], 	Loss: 1.8870
[ 71 / 250], 	Loss: 1.9325
[ 72 / 250], 	Loss: 1.9363
[ 73 / 250], 	Loss: 1.8870
[ 74 / 250], 	Loss: 1.9109
[ 75 / 250], 	Loss: 1.8891
[ 76 / 250], 	Loss: 1.9075
[ 77 / 250], 	Loss: 1.9506
[ 78 / 250], 	Loss: 2.0138
[ 79 / 250], 	Loss: 2.0386
[ 80 / 250], 	Loss: 1.9950
[ 81 / 250], 	Loss: 1.9186
[ 82 / 250], 	Loss: 1.9383
[ 83 / 250], 	Loss: 1.9607
[ 84 / 250], 	Loss: 2.1035
[ 85 / 250], 	Loss: 1.9625
[ 86 / 250], 	Loss: 1.8932
[ 87 / 250], 	Loss: 1.9056
[ 88 / 250], 	Loss: 1.9157
[ 89 / 250], 	Loss: 1.8898
[ 90 / 250], 	Loss: 2.0166
[ 91 / 250], 	Loss: 1.9590
[ 92 / 250], 	Loss: 1.8949
[ 93 / 250], 	Loss: 1.8761
[ 94 / 250], 	Loss: 1.8722
[ 95 / 250], 	Loss: 1.8637
[ 96 / 250], 	Loss: 1.9098
[ 97 / 250], 	Loss: 1.8790
[ 98 / 250], 	Loss: 1.9685
[ 99 / 250], 	Loss: 1.8836
[100 / 250], 	Loss: 1.8617
[101 / 250], 	Loss: 1.8442
[102 / 250], 	Loss: 1.8274
[103 / 250], 	Loss: 1.8329
[104 / 250], 	Loss: 1.8283
[105 / 250], 	Loss: 1.8275
[106 / 250], 	Loss: 1.8310
[107 / 250], 	Loss: 1.8286
[108 / 250], 	Loss: 1.8272
[109 / 250], 	Loss: 1.8311
[110 / 250], 	Loss: 1.8262
[111 / 250], 	Loss: 1.8193
[112 / 250], 	Loss: 1.8232
[113 / 250], 	Loss: 1.8215
[114 / 250], 	Loss: 1.8188
[115 / 250], 	Loss: 1.8182
[116 / 250], 	Loss: 1.8181
[117 / 250], 	Loss: 1.8298
[118 / 250], 	Loss: 1.8218
[119 / 250], 	Loss: 1.8330
[120 / 250], 	Loss: 1.8202
[121 / 250], 	Loss: 1.8345
[122 / 250], 	Loss: 1.8265
[123 / 250], 	Loss: 1.8165
[124 / 250], 	Loss: 1.8209
[125 / 250], 	Loss: 1.8249
[126 / 250], 	Loss: 1.8168
[127 / 250], 	Loss: 1.8227
[128 / 250], 	Loss: 1.8104
[129 / 250], 	Loss: 1.8205
[130 / 250], 	Loss: 1.8083
[131 / 250], 	Loss: 1.8279
[132 / 250], 	Loss: 1.8240
[133 / 250], 	Loss: 1.7946
[134 / 250], 	Loss: 1.8069
[135 / 250], 	Loss: 1.8104
[136 / 250], 	Loss: 1.8146
[137 / 250], 	Loss: 1.8136
[138 / 250], 	Loss: 1.8080
[139 / 250], 	Loss: 1.8079
[140 / 250], 	Loss: 1.8184
[141 / 250], 	Loss: 1.8159
[142 / 250], 	Loss: 1.8119
[143 / 250], 	Loss: 1.8119
[144 / 250], 	Loss: 1.8081
[145 / 250], 	Loss: 1.8228
[146 / 250], 	Loss: 1.8179
[147 / 250], 	Loss: 1.8234
[148 / 250], 	Loss: 1.8174
[149 / 250], 	Loss: 1.8119
[150 / 250], 	Loss: 1.8106
[151 / 250], 	Loss: 1.8208
[152 / 250], 	Loss: 1.7993
[153 / 250], 	Loss: 1.7946
[154 / 250], 	Loss: 1.8023
[155 / 250], 	Loss: 1.8088
[156 / 250], 	Loss: 1.8152
[157 / 250], 	Loss: 1.8021
[158 / 250], 	Loss: 1.8033
[159 / 250], 	Loss: 1.8221
[160 / 250], 	Loss: 1.8189
[161 / 250], 	Loss: 1.8186
[162 / 250], 	Loss: 1.8144
[163 / 250], 	Loss: 1.8099
[164 / 250], 	Loss: 1.8035
[165 / 250], 	Loss: 1.8079
[166 / 250], 	Loss: 1.8193
[167 / 250], 	Loss: 1.8124
[168 / 250], 	Loss: 1.8104
[169 / 250], 	Loss: 1.8125
[170 / 250], 	Loss: 1.8128
[171 / 250], 	Loss: 1.8057
[172 / 250], 	Loss: 1.8143
[173 / 250], 	Loss: 1.8081
[174 / 250], 	Loss: 1.8156
[175 / 250], 	Loss: 1.8044
[176 / 250], 	Loss: 1.8037
[177 / 250], 	Loss: 1.8269
[178 / 250], 	Loss: 1.8180
[179 / 250], 	Loss: 1.8042
[180 / 250], 	Loss: 1.8077
[181 / 250], 	Loss: 1.8042
[182 / 250], 	Loss: 1.8088
[183 / 250], 	Loss: 1.7988
[184 / 250], 	Loss: 1.8001
[185 / 250], 	Loss: 1.8091
[186 / 250], 	Loss: 1.7992
[187 / 250], 	Loss: 1.8092
[188 / 250], 	Loss: 1.8074
[189 / 250], 	Loss: 1.8129
[190 / 250], 	Loss: 1.7937
[191 / 250], 	Loss: 1.8120
[192 / 250], 	Loss: 1.7967
[193 / 250], 	Loss: 1.8069
[194 / 250], 	Loss: 1.7990
[195 / 250], 	Loss: 1.7969
[196 / 250], 	Loss: 1.8011
[197 / 250], 	Loss: 1.7970
[198 / 250], 	Loss: 1.7915
[199 / 250], 	Loss: 1.8078
[200 / 250], 	Loss: 1.8098
[201 / 250], 	Loss: 1.8170
[202 / 250], 	Loss: 1.8327
[203 / 250], 	Loss: 1.8032
[204 / 250], 	Loss: 1.8060
[205 / 250], 	Loss: 1.8052
[206 / 250], 	Loss: 1.7930
[207 / 250], 	Loss: 1.8037
[208 / 250], 	Loss: 1.8066
[209 / 250], 	Loss: 1.7961
[210 / 250], 	Loss: 1.8131
[211 / 250], 	Loss: 1.8102
[212 / 250], 	Loss: 1.7975
[213 / 250], 	Loss: 1.8139
[214 / 250], 	Loss: 1.8143
[215 / 250], 	Loss: 1.8071
[216 / 250], 	Loss: 1.7988
[217 / 250], 	Loss: 1.8131
[218 / 250], 	Loss: 1.8070
[219 / 250], 	Loss: 1.8055
[220 / 250], 	Loss: 1.7987
[221 / 250], 	Loss: 1.8213
[222 / 250], 	Loss: 1.8055
[223 / 250], 	Loss: 1.8108
[224 / 250], 	Loss: 1.8115
[225 / 250], 	Loss: 1.8006
[226 / 250], 	Loss: 1.8054
[227 / 250], 	Loss: 1.8074
[228 / 250], 	Loss: 1.8077
[229 / 250], 	Loss: 1.8054
[230 / 250], 	Loss: 1.8011
[231 / 250], 	Loss: 1.8174
[232 / 250], 	Loss: 1.8052
[233 / 250], 	Loss: 1.7947
[234 / 250], 	Loss: 1.8097
[235 / 250], 	Loss: 1.8021
[236 / 250], 	Loss: 1.8144
[237 / 250], 	Loss: 1.7972
[238 / 250], 	Loss: 1.7965
[239 / 250], 	Loss: 1.8032
[240 / 250], 	Loss: 1.8020
[241 / 250], 	Loss: 1.8046
[242 / 250], 	Loss: 1.7993
[243 / 250], 	Loss: 1.8101
[244 / 250], 	Loss: 1.7990
[245 / 250], 	Loss: 1.8027
[246 / 250], 	Loss: 1.8052
[247 / 250], 	Loss: 1.8019
[248 / 250], 	Loss: 1.8085
[249 / 250], 	Loss: 1.8050
[250 / 250], 	Loss: 1.8048

The accuracy of the 1st task is: 6.4000%
The accuracy of current task 5 is: 69.3000%
The average accuracy is: 33.9800%

------------------------------ Task 5 Training End ------------------------------


------------------------------ Task 6 Training Begin ------------------------------

The number of train set: 5000. The number of test set: 1000

# new classes:  10
[  1 / 250], 	Loss: 3.2530
[  2 / 250], 	Loss: 2.8785
[  3 / 250], 	Loss: 2.7697
[  4 / 250], 	Loss: 2.7300
[  5 / 250], 	Loss: 2.6486
[  6 / 250], 	Loss: 2.6111
[  7 / 250], 	Loss: 2.5849
[  8 / 250], 	Loss: 2.5293
[  9 / 250], 	Loss: 2.5139
[ 10 / 250], 	Loss: 2.4609
[ 11 / 250], 	Loss: 2.4428
[ 12 / 250], 	Loss: 2.4215
[ 13 / 250], 	Loss: 2.3619
[ 14 / 250], 	Loss: 2.3499
[ 15 / 250], 	Loss: 2.3324
[ 16 / 250], 	Loss: 2.3055
[ 17 / 250], 	Loss: 2.3182
[ 18 / 250], 	Loss: 2.2992
[ 19 / 250], 	Loss: 2.2691
[ 20 / 250], 	Loss: 2.2857
[ 21 / 250], 	Loss: 2.2120
[ 22 / 250], 	Loss: 2.2393
[ 23 / 250], 	Loss: 2.1902
[ 24 / 250], 	Loss: 2.1933
[ 25 / 250], 	Loss: 2.1909
[ 26 / 250], 	Loss: 2.1768
[ 27 / 250], 	Loss: 2.1781
[ 28 / 250], 	Loss: 2.1833
[ 29 / 250], 	Loss: 2.1876
[ 30 / 250], 	Loss: 2.2077
[ 31 / 250], 	Loss: 2.1411
[ 32 / 250], 	Loss: 2.1868
[ 33 / 250], 	Loss: 2.1463
[ 34 / 250], 	Loss: 2.1404
[ 35 / 250], 	Loss: 2.1143
[ 36 / 250], 	Loss: 2.1090
[ 37 / 250], 	Loss: 2.1156
[ 38 / 250], 	Loss: 2.1668
[ 39 / 250], 	Loss: 2.1004
[ 40 / 250], 	Loss: 2.0646
[ 41 / 250], 	Loss: 2.0634
[ 42 / 250], 	Loss: 2.0832
[ 43 / 250], 	Loss: 2.0798
[ 44 / 250], 	Loss: 2.0695
[ 45 / 250], 	Loss: 2.0811
[ 46 / 250], 	Loss: 2.1015
[ 47 / 250], 	Loss: 2.0905
[ 48 / 250], 	Loss: 2.0523
[ 49 / 250], 	Loss: 2.0423
[ 50 / 250], 	Loss: 2.0322
[ 51 / 250], 	Loss: 2.0891
[ 52 / 250], 	Loss: 2.0346
[ 53 / 250], 	Loss: 2.0925
[ 54 / 250], 	Loss: 2.0326
[ 55 / 250], 	Loss: 2.0365
[ 56 / 250], 	Loss: 2.0172
[ 57 / 250], 	Loss: 2.0189
[ 58 / 250], 	Loss: 2.0163
[ 59 / 250], 	Loss: 2.0462
[ 60 / 250], 	Loss: 2.0526
[ 61 / 250], 	Loss: 2.0969
[ 62 / 250], 	Loss: 2.0585
[ 63 / 250], 	Loss: 2.1320
[ 64 / 250], 	Loss: 2.0540
[ 65 / 250], 	Loss: 2.0401
[ 66 / 250], 	Loss: 2.0657
[ 67 / 250], 	Loss: 2.0167
[ 68 / 250], 	Loss: 2.0156
[ 69 / 250], 	Loss: 2.1536
[ 70 / 250], 	Loss: 2.1046
[ 71 / 250], 	Loss: 2.0791
[ 72 / 250], 	Loss: 2.0283
[ 73 / 250], 	Loss: 2.0749
[ 74 / 250], 	Loss: 2.1444
[ 75 / 250], 	Loss: 2.0936
[ 76 / 250], 	Loss: 2.0411
[ 77 / 250], 	Loss: 2.0677
[ 78 / 250], 	Loss: 2.0217
[ 79 / 250], 	Loss: 2.0011
[ 80 / 250], 	Loss: 2.0056
[ 81 / 250], 	Loss: 1.9974
[ 82 / 250], 	Loss: 2.0150
[ 83 / 250], 	Loss: 2.0366
[ 84 / 250], 	Loss: 2.1178
[ 85 / 250], 	Loss: 2.0136
[ 86 / 250], 	Loss: 2.0076
[ 87 / 250], 	Loss: 2.0674
[ 88 / 250], 	Loss: 2.0586
[ 89 / 250], 	Loss: 2.1231
[ 90 / 250], 	Loss: 2.0246
[ 91 / 250], 	Loss: 2.1378
[ 92 / 250], 	Loss: 2.0210
[ 93 / 250], 	Loss: 2.0001
[ 94 / 250], 	Loss: 2.0315
[ 95 / 250], 	Loss: 2.0149
[ 96 / 250], 	Loss: 2.1481
[ 97 / 250], 	Loss: 2.0661
[ 98 / 250], 	Loss: 1.9996
[ 99 / 250], 	Loss: 1.9984
[100 / 250], 	Loss: 2.0150
[101 / 250], 	Loss: 1.9771
[102 / 250], 	Loss: 1.9644
[103 / 250], 	Loss: 1.9583
[104 / 250], 	Loss: 1.9561
[105 / 250], 	Loss: 1.9662
[106 / 250], 	Loss: 1.9514
[107 / 250], 	Loss: 1.9506
[108 / 250], 	Loss: 1.9633
[109 / 250], 	Loss: 1.9405
[110 / 250], 	Loss: 1.9421
[111 / 250], 	Loss: 1.9483
[112 / 250], 	Loss: 1.9470
[113 / 250], 	Loss: 1.9485
[114 / 250], 	Loss: 1.9439
[115 / 250], 	Loss: 1.9400
[116 / 250], 	Loss: 1.9464
[117 / 250], 	Loss: 1.9407
[118 / 250], 	Loss: 1.9427
[119 / 250], 	Loss: 1.9405
[120 / 250], 	Loss: 1.9394
[121 / 250], 	Loss: 1.9430
[122 / 250], 	Loss: 1.9553
[123 / 250], 	Loss: 1.9234
[124 / 250], 	Loss: 1.9435
[125 / 250], 	Loss: 1.9441
[126 / 250], 	Loss: 1.9358
[127 / 250], 	Loss: 1.9389
[128 / 250], 	Loss: 1.9223
[129 / 250], 	Loss: 1.9363
[130 / 250], 	Loss: 1.9293
[131 / 250], 	Loss: 1.9469
[132 / 250], 	Loss: 1.9314
[133 / 250], 	Loss: 1.9453
[134 / 250], 	Loss: 1.9349
[135 / 250], 	Loss: 1.9464
[136 / 250], 	Loss: 1.9399
[137 / 250], 	Loss: 1.9386
[138 / 250], 	Loss: 1.9385
[139 / 250], 	Loss: 1.9205
[140 / 250], 	Loss: 1.9415
[141 / 250], 	Loss: 1.9279
[142 / 250], 	Loss: 1.9193
[143 / 250], 	Loss: 1.9367
[144 / 250], 	Loss: 1.9318
[145 / 250], 	Loss: 1.9273
[146 / 250], 	Loss: 1.9413
[147 / 250], 	Loss: 1.9309
[148 / 250], 	Loss: 1.9292
[149 / 250], 	Loss: 1.9282
[150 / 250], 	Loss: 1.9176
[151 / 250], 	Loss: 1.9280
[152 / 250], 	Loss: 1.9343
[153 / 250], 	Loss: 1.9268
[154 / 250], 	Loss: 1.9335
[155 / 250], 	Loss: 1.9312
[156 / 250], 	Loss: 1.9277
[157 / 250], 	Loss: 1.9414
[158 / 250], 	Loss: 1.9289
[159 / 250], 	Loss: 1.9259
[160 / 250], 	Loss: 1.9308
[161 / 250], 	Loss: 1.9381
[162 / 250], 	Loss: 1.9259
[163 / 250], 	Loss: 1.9241
[164 / 250], 	Loss: 1.9403
[165 / 250], 	Loss: 1.9182
[166 / 250], 	Loss: 1.9189
[167 / 250], 	Loss: 1.9408
[168 / 250], 	Loss: 1.9216
[169 / 250], 	Loss: 1.9340
[170 / 250], 	Loss: 1.9351
[171 / 250], 	Loss: 1.9374
[172 / 250], 	Loss: 1.9314
[173 / 250], 	Loss: 1.9242
[174 / 250], 	Loss: 1.9200
[175 / 250], 	Loss: 1.9265
[176 / 250], 	Loss: 1.9184
[177 / 250], 	Loss: 1.9233
[178 / 250], 	Loss: 1.9371
[179 / 250], 	Loss: 1.9181
[180 / 250], 	Loss: 1.9125
[181 / 250], 	Loss: 1.9282
[182 / 250], 	Loss: 1.9359
[183 / 250], 	Loss: 1.9185
[184 / 250], 	Loss: 1.9292
[185 / 250], 	Loss: 1.9217
[186 / 250], 	Loss: 1.9178
[187 / 250], 	Loss: 1.9339
[188 / 250], 	Loss: 1.9209
[189 / 250], 	Loss: 1.9264
[190 / 250], 	Loss: 1.9294
[191 / 250], 	Loss: 1.9272
[192 / 250], 	Loss: 1.9239
[193 / 250], 	Loss: 1.9221
[194 / 250], 	Loss: 1.9277
[195 / 250], 	Loss: 1.9163
[196 / 250], 	Loss: 1.9322
[197 / 250], 	Loss: 1.9372
[198 / 250], 	Loss: 1.9159
[199 / 250], 	Loss: 1.9317
[200 / 250], 	Loss: 1.9217
[201 / 250], 	Loss: 1.9325
[202 / 250], 	Loss: 1.9373
[203 / 250], 	Loss: 1.9122
[204 / 250], 	Loss: 1.9228
[205 / 250], 	Loss: 1.9264
[206 / 250], 	Loss: 1.9220
[207 / 250], 	Loss: 1.9173
[208 / 250], 	Loss: 1.9311
[209 / 250], 	Loss: 1.9293
[210 / 250], 	Loss: 1.9305
[211 / 250], 	Loss: 1.9322
[212 / 250], 	Loss: 1.9188
[213 / 250], 	Loss: 1.9313
[214 / 250], 	Loss: 1.9332
[215 / 250], 	Loss: 1.9174
[216 / 250], 	Loss: 1.9210
[217 / 250], 	Loss: 1.9173
[218 / 250], 	Loss: 1.9159
[219 / 250], 	Loss: 1.9185
[220 / 250], 	Loss: 1.9216
[221 / 250], 	Loss: 1.9184
[222 / 250], 	Loss: 1.9339
[223 / 250], 	Loss: 1.9291
[224 / 250], 	Loss: 1.9068
[225 / 250], 	Loss: 1.9288
[226 / 250], 	Loss: 1.9321
[227 / 250], 	Loss: 1.9199
[228 / 250], 	Loss: 1.9209
[229 / 250], 	Loss: 1.9134
[230 / 250], 	Loss: 1.9240
[231 / 250], 	Loss: 1.9195
[232 / 250], 	Loss: 1.9447
[233 / 250], 	Loss: 1.9380
[234 / 250], 	Loss: 1.9322
[235 / 250], 	Loss: 1.9185
[236 / 250], 	Loss: 1.9276
[237 / 250], 	Loss: 1.9259
[238 / 250], 	Loss: 1.9214
[239 / 250], 	Loss: 1.9300
[240 / 250], 	Loss: 1.9269
[241 / 250], 	Loss: 1.9299
[242 / 250], 	Loss: 1.9274
[243 / 250], 	Loss: 1.9214
[244 / 250], 	Loss: 1.9087
[245 / 250], 	Loss: 1.9242
[246 / 250], 	Loss: 1.9209
[247 / 250], 	Loss: 1.9360
[248 / 250], 	Loss: 1.9249
[249 / 250], 	Loss: 1.9236
[250 / 250], 	Loss: 1.9278

The accuracy of the 1st task is: 2.3000%
The accuracy of current task 6 is: 67.6000%
The average accuracy is: 29.5000%

------------------------------ Task 6 Training End ------------------------------


------------------------------ Task 7 Training Begin ------------------------------

The number of train set: 5000. The number of test set: 1000

# new classes:  10
[  1 / 250], 	Loss: 2.8263
[  2 / 250], 	Loss: 2.4282
[  3 / 250], 	Loss: 2.3578
[  4 / 250], 	Loss: 2.2875
[  5 / 250], 	Loss: 2.2777
[  6 / 250], 	Loss: 2.2377
[  7 / 250], 	Loss: 2.1861
[  8 / 250], 	Loss: 2.1688
[  9 / 250], 	Loss: 2.1404
[ 10 / 250], 	Loss: 2.1265
[ 11 / 250], 	Loss: 2.1153
[ 12 / 250], 	Loss: 2.1309
[ 13 / 250], 	Loss: 2.1179
[ 14 / 250], 	Loss: 2.0850
[ 15 / 250], 	Loss: 2.0549
[ 16 / 250], 	Loss: 2.0407
[ 17 / 250], 	Loss: 2.0483
[ 18 / 250], 	Loss: 2.0477
[ 19 / 250], 	Loss: 2.0170
[ 20 / 250], 	Loss: 2.0145
[ 21 / 250], 	Loss: 1.9830
[ 22 / 250], 	Loss: 1.9964
[ 23 / 250], 	Loss: 1.9772
[ 24 / 250], 	Loss: 2.0386
[ 25 / 250], 	Loss: 1.9966
[ 26 / 250], 	Loss: 1.9801
[ 27 / 250], 	Loss: 1.9826
[ 28 / 250], 	Loss: 1.9798
[ 29 / 250], 	Loss: 1.9540
[ 30 / 250], 	Loss: 1.9534
[ 31 / 250], 	Loss: 1.9597
[ 32 / 250], 	Loss: 1.9351
[ 33 / 250], 	Loss: 1.9547
[ 34 / 250], 	Loss: 1.9523
[ 35 / 250], 	Loss: 1.9414
[ 36 / 250], 	Loss: 1.9489
[ 37 / 250], 	Loss: 1.9498
[ 38 / 250], 	Loss: 1.9782
[ 39 / 250], 	Loss: 1.9442
[ 40 / 250], 	Loss: 2.0240
[ 41 / 250], 	Loss: 1.9479
[ 42 / 250], 	Loss: 1.9327
[ 43 / 250], 	Loss: 1.9396
[ 44 / 250], 	Loss: 1.9385
[ 45 / 250], 	Loss: 1.9548
[ 46 / 250], 	Loss: 1.9428
[ 47 / 250], 	Loss: 1.9207
[ 48 / 250], 	Loss: 1.9231
[ 49 / 250], 	Loss: 1.9495
[ 50 / 250], 	Loss: 1.9884
[ 51 / 250], 	Loss: 1.9505
[ 52 / 250], 	Loss: 2.0180
[ 53 / 250], 	Loss: 1.9367
[ 54 / 250], 	Loss: 1.9390
[ 55 / 250], 	Loss: 2.0148
[ 56 / 250], 	Loss: 1.9611
[ 57 / 250], 	Loss: 1.9397
[ 58 / 250], 	Loss: 1.9224
[ 59 / 250], 	Loss: 1.9854
[ 60 / 250], 	Loss: 1.9874
[ 61 / 250], 	Loss: 1.9437
[ 62 / 250], 	Loss: 1.9461
[ 63 / 250], 	Loss: 1.9325
[ 64 / 250], 	Loss: 1.9351
[ 65 / 250], 	Loss: 1.9083
[ 66 / 250], 	Loss: 1.9092
[ 67 / 250], 	Loss: 1.9098
[ 68 / 250], 	Loss: 1.9079
[ 69 / 250], 	Loss: 1.9369
[ 70 / 250], 	Loss: 1.9194
[ 71 / 250], 	Loss: 1.9172
[ 72 / 250], 	Loss: 1.9111
[ 73 / 250], 	Loss: 1.9761
[ 74 / 250], 	Loss: 1.9454
[ 75 / 250], 	Loss: 1.9244
[ 76 / 250], 	Loss: 1.9350
[ 77 / 250], 	Loss: 1.9080
[ 78 / 250], 	Loss: 1.9007
[ 79 / 250], 	Loss: 1.9126
[ 80 / 250], 	Loss: 1.9083
[ 81 / 250], 	Loss: 1.9371
[ 82 / 250], 	Loss: 1.9183
[ 83 / 250], 	Loss: 1.9059
[ 84 / 250], 	Loss: 1.9005
[ 85 / 250], 	Loss: 1.9028
[ 86 / 250], 	Loss: 1.9041
[ 87 / 250], 	Loss: 1.9093
[ 88 / 250], 	Loss: 1.9028
[ 89 / 250], 	Loss: 1.8863
[ 90 / 250], 	Loss: 1.8835
[ 91 / 250], 	Loss: 1.8881
[ 92 / 250], 	Loss: 1.8861
[ 93 / 250], 	Loss: 1.8904
[ 94 / 250], 	Loss: 1.9007
[ 95 / 250], 	Loss: 1.8995
[ 96 / 250], 	Loss: 1.9096
[ 97 / 250], 	Loss: 1.9024
[ 98 / 250], 	Loss: 1.8841
[ 99 / 250], 	Loss: 1.9354
[100 / 250], 	Loss: 1.9170
[101 / 250], 	Loss: 1.8882
[102 / 250], 	Loss: 1.8854
[103 / 250], 	Loss: 1.8626
[104 / 250], 	Loss: 1.8708
[105 / 250], 	Loss: 1.8782
[106 / 250], 	Loss: 1.8746
[107 / 250], 	Loss: 1.8791
[108 / 250], 	Loss: 1.8771
[109 / 250], 	Loss: 1.8727
[110 / 250], 	Loss: 1.8851
[111 / 250], 	Loss: 1.8699
[112 / 250], 	Loss: 1.8766
[113 / 250], 	Loss: 1.8652
[114 / 250], 	Loss: 1.8672
[115 / 250], 	Loss: 1.8730
[116 / 250], 	Loss: 1.8665
[117 / 250], 	Loss: 1.8665
[118 / 250], 	Loss: 1.8823
[119 / 250], 	Loss: 1.8771
[120 / 250], 	Loss: 1.8863
[121 / 250], 	Loss: 1.8728
[122 / 250], 	Loss: 1.8646
[123 / 250], 	Loss: 1.8581
[124 / 250], 	Loss: 1.8722
[125 / 250], 	Loss: 1.8666
[126 / 250], 	Loss: 1.8603
[127 / 250], 	Loss: 1.8702
[128 / 250], 	Loss: 1.8703
[129 / 250], 	Loss: 1.8537
[130 / 250], 	Loss: 1.8820
[131 / 250], 	Loss: 1.8652
[132 / 250], 	Loss: 1.8686
[133 / 250], 	Loss: 1.8770
[134 / 250], 	Loss: 1.8744
[135 / 250], 	Loss: 1.8667
[136 / 250], 	Loss: 1.8710
[137 / 250], 	Loss: 1.8673
[138 / 250], 	Loss: 1.8697
[139 / 250], 	Loss: 1.8595
[140 / 250], 	Loss: 1.8611
[141 / 250], 	Loss: 1.8680
[142 / 250], 	Loss: 1.8693
[143 / 250], 	Loss: 1.8715
[144 / 250], 	Loss: 1.8597
[145 / 250], 	Loss: 1.8702
[146 / 250], 	Loss: 1.8751
[147 / 250], 	Loss: 1.8744
[148 / 250], 	Loss: 1.8656
[149 / 250], 	Loss: 1.8622
[150 / 250], 	Loss: 1.8678
[151 / 250], 	Loss: 1.8672
[152 / 250], 	Loss: 1.8687
[153 / 250], 	Loss: 1.8752
[154 / 250], 	Loss: 1.8609
[155 / 250], 	Loss: 1.8565
[156 / 250], 	Loss: 1.8701
[157 / 250], 	Loss: 1.8553
[158 / 250], 	Loss: 1.8708
[159 / 250], 	Loss: 1.8663
[160 / 250], 	Loss: 1.8604
[161 / 250], 	Loss: 1.8668
[162 / 250], 	Loss: 1.8753
[163 / 250], 	Loss: 1.8787
[164 / 250], 	Loss: 1.8609
[165 / 250], 	Loss: 1.8605
[166 / 250], 	Loss: 1.8614
[167 / 250], 	Loss: 1.8496
[168 / 250], 	Loss: 1.8701
[169 / 250], 	Loss: 1.8803
[170 / 250], 	Loss: 1.8591
[171 / 250], 	Loss: 1.8619
[172 / 250], 	Loss: 1.8637
[173 / 250], 	Loss: 1.8681
[174 / 250], 	Loss: 1.8655
[175 / 250], 	Loss: 1.8598
[176 / 250], 	Loss: 1.8513
[177 / 250], 	Loss: 1.8559
[178 / 250], 	Loss: 1.8534
[179 / 250], 	Loss: 1.8665
[180 / 250], 	Loss: 1.8633
[181 / 250], 	Loss: 1.8636
[182 / 250], 	Loss: 1.8687
[183 / 250], 	Loss: 1.8558
[184 / 250], 	Loss: 1.8594
[185 / 250], 	Loss: 1.8579
[186 / 250], 	Loss: 1.8623
[187 / 250], 	Loss: 1.8610
[188 / 250], 	Loss: 1.8690
[189 / 250], 	Loss: 1.8553
[190 / 250], 	Loss: 1.8590
[191 / 250], 	Loss: 1.8655
[192 / 250], 	Loss: 1.8569
[193 / 250], 	Loss: 1.8644
[194 / 250], 	Loss: 1.8443
[195 / 250], 	Loss: 1.8504
[196 / 250], 	Loss: 1.8715
[197 / 250], 	Loss: 1.8606
[198 / 250], 	Loss: 1.8721
[199 / 250], 	Loss: 1.8602
[200 / 250], 	Loss: 1.8567
[201 / 250], 	Loss: 1.8652
[202 / 250], 	Loss: 1.8615
[203 / 250], 	Loss: 1.8466
[204 / 250], 	Loss: 1.8702
[205 / 250], 	Loss: 1.8508
[206 / 250], 	Loss: 1.8470
[207 / 250], 	Loss: 1.8624
[208 / 250], 	Loss: 1.8544
[209 / 250], 	Loss: 1.8656
[210 / 250], 	Loss: 1.8589
[211 / 250], 	Loss: 1.8540
[212 / 250], 	Loss: 1.8541
[213 / 250], 	Loss: 1.8667
[214 / 250], 	Loss: 1.8779
[215 / 250], 	Loss: 1.8490
[216 / 250], 	Loss: 1.8601
[217 / 250], 	Loss: 1.8646
[218 / 250], 	Loss: 1.8676
[219 / 250], 	Loss: 1.8577
[220 / 250], 	Loss: 1.8721
[221 / 250], 	Loss: 1.8667
[222 / 250], 	Loss: 1.8595
[223 / 250], 	Loss: 1.8732
[224 / 250], 	Loss: 1.8730
[225 / 250], 	Loss: 1.8622
[226 / 250], 	Loss: 1.8674
[227 / 250], 	Loss: 1.8586
[228 / 250], 	Loss: 1.8542
[229 / 250], 	Loss: 1.8692
[230 / 250], 	Loss: 1.8568
[231 / 250], 	Loss: 1.8606
[232 / 250], 	Loss: 1.8699
[233 / 250], 	Loss: 1.8622
[234 / 250], 	Loss: 1.8672
[235 / 250], 	Loss: 1.8502
[236 / 250], 	Loss: 1.8645
[237 / 250], 	Loss: 1.8679
[238 / 250], 	Loss: 1.8544
[239 / 250], 	Loss: 1.8598
[240 / 250], 	Loss: 1.8683
[241 / 250], 	Loss: 1.8481
[242 / 250], 	Loss: 1.8548
[243 / 250], 	Loss: 1.8855
[244 / 250], 	Loss: 1.8622
[245 / 250], 	Loss: 1.8544
[246 / 250], 	Loss: 1.8715
[247 / 250], 	Loss: 1.8691
[248 / 250], 	Loss: 1.8607
[249 / 250], 	Loss: 1.8577
[250 / 250], 	Loss: 1.8653

The accuracy of the 1st task is: 1.1000%
The accuracy of current task 7 is: 78.0000%
The average accuracy is: 28.8000%

------------------------------ Task 7 Training End ------------------------------


------------------------------ Task 8 Training Begin ------------------------------

The number of train set: 5000. The number of test set: 1000

# new classes:  10
[  1 / 250], 	Loss: 3.0603
[  2 / 250], 	Loss: 2.6041
[  3 / 250], 	Loss: 2.5240
[  4 / 250], 	Loss: 2.4659
[  5 / 250], 	Loss: 2.4452
[  6 / 250], 	Loss: 2.3938
[  7 / 250], 	Loss: 2.3510
[  8 / 250], 	Loss: 2.3148
[  9 / 250], 	Loss: 2.2535
[ 10 / 250], 	Loss: 2.2507
[ 11 / 250], 	Loss: 2.2444
[ 12 / 250], 	Loss: 2.2051
[ 13 / 250], 	Loss: 2.1915
[ 14 / 250], 	Loss: 2.1612
[ 15 / 250], 	Loss: 2.1520
[ 16 / 250], 	Loss: 2.1421
[ 17 / 250], 	Loss: 2.1312
[ 18 / 250], 	Loss: 2.1445
[ 19 / 250], 	Loss: 2.2147
[ 20 / 250], 	Loss: 2.1560
[ 21 / 250], 	Loss: 2.1655
[ 22 / 250], 	Loss: 2.1570
[ 23 / 250], 	Loss: 2.1167
[ 24 / 250], 	Loss: 2.1104
[ 25 / 250], 	Loss: 2.0996
[ 26 / 250], 	Loss: 2.2122
[ 27 / 250], 	Loss: 2.0963
[ 28 / 250], 	Loss: 2.0749
[ 29 / 250], 	Loss: 2.1267
[ 30 / 250], 	Loss: 2.1237
[ 31 / 250], 	Loss: 2.0761
[ 32 / 250], 	Loss: 2.0702
[ 33 / 250], 	Loss: 2.0706
[ 34 / 250], 	Loss: 2.0890
[ 35 / 250], 	Loss: 2.0485
[ 36 / 250], 	Loss: 2.0280
[ 37 / 250], 	Loss: 2.0397
[ 38 / 250], 	Loss: 2.0461
[ 39 / 250], 	Loss: 2.1122
[ 40 / 250], 	Loss: 2.0535
[ 41 / 250], 	Loss: 2.0383
[ 42 / 250], 	Loss: 2.0331
[ 43 / 250], 	Loss: 2.0288
[ 44 / 250], 	Loss: 2.0183
[ 45 / 250], 	Loss: 2.0159
[ 46 / 250], 	Loss: 2.0514
[ 47 / 250], 	Loss: 2.0304
[ 48 / 250], 	Loss: 2.0831
[ 49 / 250], 	Loss: 2.0359
[ 50 / 250], 	Loss: 2.0458
[ 51 / 250], 	Loss: 2.0287
[ 52 / 250], 	Loss: 2.0121
[ 53 / 250], 	Loss: 2.0142
[ 54 / 250], 	Loss: 2.0203
[ 55 / 250], 	Loss: 2.0059
[ 56 / 250], 	Loss: 2.0028
[ 57 / 250], 	Loss: 2.0154
[ 58 / 250], 	Loss: 2.0081
[ 59 / 250], 	Loss: 2.0265
[ 60 / 250], 	Loss: 2.0786
[ 61 / 250], 	Loss: 2.0976
[ 62 / 250], 	Loss: 2.0364
[ 63 / 250], 	Loss: 2.0160
[ 64 / 250], 	Loss: 2.0151
[ 65 / 250], 	Loss: 2.0018
[ 66 / 250], 	Loss: 1.9996
[ 67 / 250], 	Loss: 1.9953
[ 68 / 250], 	Loss: 2.0090
[ 69 / 250], 	Loss: 2.0091
[ 70 / 250], 	Loss: 1.9982
[ 71 / 250], 	Loss: 2.0709
[ 72 / 250], 	Loss: 2.0383
[ 73 / 250], 	Loss: 2.0201
[ 74 / 250], 	Loss: 2.0681
[ 75 / 250], 	Loss: 2.0275
[ 76 / 250], 	Loss: 2.0523
[ 77 / 250], 	Loss: 2.0391
[ 78 / 250], 	Loss: 2.0095
[ 79 / 250], 	Loss: 2.0743
[ 80 / 250], 	Loss: 2.0233
[ 81 / 250], 	Loss: 2.0219
[ 82 / 250], 	Loss: 2.0508
[ 83 / 250], 	Loss: 2.0457
[ 84 / 250], 	Loss: 2.0212
[ 85 / 250], 	Loss: 2.0077
[ 86 / 250], 	Loss: 2.0052
[ 87 / 250], 	Loss: 2.0145
[ 88 / 250], 	Loss: 2.0292
[ 89 / 250], 	Loss: 2.0667
[ 90 / 250], 	Loss: 2.0045
[ 91 / 250], 	Loss: 2.0005
[ 92 / 250], 	Loss: 1.9998
[ 93 / 250], 	Loss: 2.0013
[ 94 / 250], 	Loss: 2.0617
[ 95 / 250], 	Loss: 2.0869
[ 96 / 250], 	Loss: 2.0052
[ 97 / 250], 	Loss: 2.0052
[ 98 / 250], 	Loss: 2.0326
[ 99 / 250], 	Loss: 2.0452
[100 / 250], 	Loss: 2.0014
[101 / 250], 	Loss: 1.9785
[102 / 250], 	Loss: 1.9781
[103 / 250], 	Loss: 1.9662
[104 / 250], 	Loss: 1.9782
[105 / 250], 	Loss: 1.9691
[106 / 250], 	Loss: 1.9767
[107 / 250], 	Loss: 1.9676
[108 / 250], 	Loss: 1.9738
[109 / 250], 	Loss: 1.9799
[110 / 250], 	Loss: 1.9624
[111 / 250], 	Loss: 1.9725
[112 / 250], 	Loss: 1.9703
[113 / 250], 	Loss: 1.9652
[114 / 250], 	Loss: 1.9718
[115 / 250], 	Loss: 1.9677
[116 / 250], 	Loss: 1.9632
[117 / 250], 	Loss: 1.9565
[118 / 250], 	Loss: 1.9643
[119 / 250], 	Loss: 1.9665
[120 / 250], 	Loss: 1.9579
[121 / 250], 	Loss: 1.9567
[122 / 250], 	Loss: 1.9679
[123 / 250], 	Loss: 1.9510
[124 / 250], 	Loss: 1.9575
[125 / 250], 	Loss: 1.9574
[126 / 250], 	Loss: 1.9702
[127 / 250], 	Loss: 1.9694
[128 / 250], 	Loss: 1.9611
[129 / 250], 	Loss: 1.9584
[130 / 250], 	Loss: 1.9537
[131 / 250], 	Loss: 1.9662
[132 / 250], 	Loss: 1.9671
[133 / 250], 	Loss: 1.9673
[134 / 250], 	Loss: 1.9636
[135 / 250], 	Loss: 1.9669
[136 / 250], 	Loss: 1.9507
[137 / 250], 	Loss: 1.9607
[138 / 250], 	Loss: 1.9552
[139 / 250], 	Loss: 1.9575
[140 / 250], 	Loss: 1.9480
[141 / 250], 	Loss: 1.9569
[142 / 250], 	Loss: 1.9502
[143 / 250], 	Loss: 1.9734
[144 / 250], 	Loss: 1.9629
[145 / 250], 	Loss: 1.9617
[146 / 250], 	Loss: 1.9575
[147 / 250], 	Loss: 1.9617
[148 / 250], 	Loss: 1.9622
[149 / 250], 	Loss: 1.9581
[150 / 250], 	Loss: 1.9606
[151 / 250], 	Loss: 1.9606
[152 / 250], 	Loss: 1.9544
[153 / 250], 	Loss: 1.9585
[154 / 250], 	Loss: 1.9690
[155 / 250], 	Loss: 1.9674
[156 / 250], 	Loss: 1.9607
[157 / 250], 	Loss: 1.9549
[158 / 250], 	Loss: 1.9646
[159 / 250], 	Loss: 1.9615
[160 / 250], 	Loss: 1.9662
[161 / 250], 	Loss: 1.9557
[162 / 250], 	Loss: 1.9576
[163 / 250], 	Loss: 1.9610
[164 / 250], 	Loss: 1.9527
[165 / 250], 	Loss: 1.9543
[166 / 250], 	Loss: 1.9591
[167 / 250], 	Loss: 1.9591
[168 / 250], 	Loss: 1.9476
[169 / 250], 	Loss: 1.9536
[170 / 250], 	Loss: 1.9516
[171 / 250], 	Loss: 1.9770
[172 / 250], 	Loss: 1.9663
[173 / 250], 	Loss: 1.9518
[174 / 250], 	Loss: 1.9505
[175 / 250], 	Loss: 1.9528
[176 / 250], 	Loss: 1.9514
[177 / 250], 	Loss: 1.9624
[178 / 250], 	Loss: 1.9503
[179 / 250], 	Loss: 1.9570
[180 / 250], 	Loss: 1.9579
[181 / 250], 	Loss: 1.9605
[182 / 250], 	Loss: 1.9531
[183 / 250], 	Loss: 1.9547
[184 / 250], 	Loss: 1.9551
[185 / 250], 	Loss: 1.9456
[186 / 250], 	Loss: 1.9511
[187 / 250], 	Loss: 1.9563
[188 / 250], 	Loss: 1.9554
[189 / 250], 	Loss: 1.9616
[190 / 250], 	Loss: 1.9599
[191 / 250], 	Loss: 1.9507
[192 / 250], 	Loss: 1.9545
[193 / 250], 	Loss: 1.9669
[194 / 250], 	Loss: 1.9499
[195 / 250], 	Loss: 1.9507
[196 / 250], 	Loss: 1.9541
[197 / 250], 	Loss: 1.9467
[198 / 250], 	Loss: 1.9582
[199 / 250], 	Loss: 1.9549
[200 / 250], 	Loss: 1.9607
[201 / 250], 	Loss: 1.9659
[202 / 250], 	Loss: 1.9593
[203 / 250], 	Loss: 1.9463
[204 / 250], 	Loss: 1.9638
[205 / 250], 	Loss: 1.9623
[206 / 250], 	Loss: 1.9570
[207 / 250], 	Loss: 1.9510
[208 / 250], 	Loss: 1.9511
[209 / 250], 	Loss: 1.9658
[210 / 250], 	Loss: 1.9552
[211 / 250], 	Loss: 1.9540
[212 / 250], 	Loss: 1.9587
[213 / 250], 	Loss: 1.9447
[214 / 250], 	Loss: 1.9567
[215 / 250], 	Loss: 1.9558
[216 / 250], 	Loss: 1.9603
[217 / 250], 	Loss: 1.9535
[218 / 250], 	Loss: 1.9527
[219 / 250], 	Loss: 1.9492
[220 / 250], 	Loss: 1.9428
[221 / 250], 	Loss: 1.9593
[222 / 250], 	Loss: 1.9550
[223 / 250], 	Loss: 1.9509
[224 / 250], 	Loss: 1.9523
[225 / 250], 	Loss: 1.9530
[226 / 250], 	Loss: 1.9547
[227 / 250], 	Loss: 1.9601
[228 / 250], 	Loss: 1.9462
[229 / 250], 	Loss: 1.9591
[230 / 250], 	Loss: 1.9504
[231 / 250], 	Loss: 1.9568
[232 / 250], 	Loss: 1.9538
[233 / 250], 	Loss: 1.9650
[234 / 250], 	Loss: 1.9574
[235 / 250], 	Loss: 1.9568
[236 / 250], 	Loss: 1.9506
[237 / 250], 	Loss: 1.9521
[238 / 250], 	Loss: 1.9530
[239 / 250], 	Loss: 1.9543
[240 / 250], 	Loss: 1.9541
[241 / 250], 	Loss: 1.9469
[242 / 250], 	Loss: 1.9585
[243 / 250], 	Loss: 1.9592
[244 / 250], 	Loss: 1.9513
[245 / 250], 	Loss: 1.9559
[246 / 250], 	Loss: 1.9557
[247 / 250], 	Loss: 1.9643
[248 / 250], 	Loss: 1.9580
[249 / 250], 	Loss: 1.9494
[250 / 250], 	Loss: 1.9553

The accuracy of the 1st task is: 0.3000%
The accuracy of current task 8 is: 69.0000%
The average accuracy is: 27.5875%

------------------------------ Task 8 Training End ------------------------------


------------------------------ Task 9 Training Begin ------------------------------

The number of train set: 5000. The number of test set: 1000

# new classes:  10
[  1 / 250], 	Loss: 3.3830
[  2 / 250], 	Loss: 2.9470
[  3 / 250], 	Loss: 2.8665
[  4 / 250], 	Loss: 2.7993
[  5 / 250], 	Loss: 2.7329
[  6 / 250], 	Loss: 2.7246
[  7 / 250], 	Loss: 2.6546
[  8 / 250], 	Loss: 2.6052
[  9 / 250], 	Loss: 2.5753
[ 10 / 250], 	Loss: 2.5800
[ 11 / 250], 	Loss: 2.5334
[ 12 / 250], 	Loss: 2.5526
[ 13 / 250], 	Loss: 2.5526
[ 14 / 250], 	Loss: 2.4940
[ 15 / 250], 	Loss: 2.4602
[ 16 / 250], 	Loss: 2.4417
[ 17 / 250], 	Loss: 2.4706
[ 18 / 250], 	Loss: 2.4864
[ 19 / 250], 	Loss: 2.4292
[ 20 / 250], 	Loss: 2.4271
[ 21 / 250], 	Loss: 2.3997
[ 22 / 250], 	Loss: 2.4058
[ 23 / 250], 	Loss: 2.3986
[ 24 / 250], 	Loss: 2.4325
[ 25 / 250], 	Loss: 2.4029
[ 26 / 250], 	Loss: 2.4050
[ 27 / 250], 	Loss: 2.3989
[ 28 / 250], 	Loss: 2.4466
[ 29 / 250], 	Loss: 2.4136
[ 30 / 250], 	Loss: 2.3930
[ 31 / 250], 	Loss: 2.3569
[ 32 / 250], 	Loss: 2.3600
[ 33 / 250], 	Loss: 2.3436
[ 34 / 250], 	Loss: 2.3604
[ 35 / 250], 	Loss: 2.3742
[ 36 / 250], 	Loss: 2.4132
[ 37 / 250], 	Loss: 2.4269
[ 38 / 250], 	Loss: 2.3845
[ 39 / 250], 	Loss: 2.3591
[ 40 / 250], 	Loss: 2.3188
[ 41 / 250], 	Loss: 2.3490
[ 42 / 250], 	Loss: 2.3388
[ 43 / 250], 	Loss: 2.3068
[ 44 / 250], 	Loss: 2.3100
[ 45 / 250], 	Loss: 2.3155
[ 46 / 250], 	Loss: 2.3816
[ 47 / 250], 	Loss: 2.3331
[ 48 / 250], 	Loss: 2.3203
[ 49 / 250], 	Loss: 2.3808
[ 50 / 250], 	Loss: 2.3060
[ 51 / 250], 	Loss: 2.3012
[ 52 / 250], 	Loss: 2.3017
[ 53 / 250], 	Loss: 2.3285
[ 54 / 250], 	Loss: 2.3739
[ 55 / 250], 	Loss: 2.3372
[ 56 / 250], 	Loss: 2.4210
[ 57 / 250], 	Loss: 2.3422
[ 58 / 250], 	Loss: 2.3290
[ 59 / 250], 	Loss: 2.3752
[ 60 / 250], 	Loss: 2.3737
[ 61 / 250], 	Loss: 2.3240
[ 62 / 250], 	Loss: 2.3224
[ 63 / 250], 	Loss: 2.3981
[ 64 / 250], 	Loss: 2.3327
[ 65 / 250], 	Loss: 2.3004
[ 66 / 250], 	Loss: 2.3068
[ 67 / 250], 	Loss: 2.3026
[ 68 / 250], 	Loss: 2.3119
[ 69 / 250], 	Loss: 2.2860
[ 70 / 250], 	Loss: 2.2918
[ 71 / 250], 	Loss: 2.2938
[ 72 / 250], 	Loss: 2.2738
[ 73 / 250], 	Loss: 2.2897
[ 74 / 250], 	Loss: 2.2941
[ 75 / 250], 	Loss: 2.4186
[ 76 / 250], 	Loss: 2.3062
[ 77 / 250], 	Loss: 2.3155
[ 78 / 250], 	Loss: 2.2972
[ 79 / 250], 	Loss: 2.3125
[ 80 / 250], 	Loss: 2.2930
[ 81 / 250], 	Loss: 2.3094
[ 82 / 250], 	Loss: 2.2926
[ 83 / 250], 	Loss: 2.2751
[ 84 / 250], 	Loss: 2.2643
[ 85 / 250], 	Loss: 2.2703
[ 86 / 250], 	Loss: 2.2673
[ 87 / 250], 	Loss: 2.2786
[ 88 / 250], 	Loss: 2.2729
[ 89 / 250], 	Loss: 2.4019
[ 90 / 250], 	Loss: 2.3136
[ 91 / 250], 	Loss: 2.2882
[ 92 / 250], 	Loss: 2.2729
[ 93 / 250], 	Loss: 2.2933
[ 94 / 250], 	Loss: 2.2585
[ 95 / 250], 	Loss: 2.2742
[ 96 / 250], 	Loss: 2.2792
[ 97 / 250], 	Loss: 2.2675
[ 98 / 250], 	Loss: 2.2689
[ 99 / 250], 	Loss: 2.2744
[100 / 250], 	Loss: 2.2642
[101 / 250], 	Loss: 2.2572
[102 / 250], 	Loss: 2.2467
[103 / 250], 	Loss: 2.2652
[104 / 250], 	Loss: 2.2497
[105 / 250], 	Loss: 2.2502
[106 / 250], 	Loss: 2.2352
[107 / 250], 	Loss: 2.2346
[108 / 250], 	Loss: 2.2303
[109 / 250], 	Loss: 2.2498
[110 / 250], 	Loss: 2.2397
[111 / 250], 	Loss: 2.2458
[112 / 250], 	Loss: 2.2411
[113 / 250], 	Loss: 2.2404
[114 / 250], 	Loss: 2.2354
[115 / 250], 	Loss: 2.2401
[116 / 250], 	Loss: 2.2316
[117 / 250], 	Loss: 2.2399
[118 / 250], 	Loss: 2.2478
[119 / 250], 	Loss: 2.2444
[120 / 250], 	Loss: 2.2297
[121 / 250], 	Loss: 2.2208
[122 / 250], 	Loss: 2.2379
[123 / 250], 	Loss: 2.2388
[124 / 250], 	Loss: 2.2240
[125 / 250], 	Loss: 2.2350
[126 / 250], 	Loss: 2.2428
[127 / 250], 	Loss: 2.2292
[128 / 250], 	Loss: 2.2418
[129 / 250], 	Loss: 2.2483
[130 / 250], 	Loss: 2.2336
[131 / 250], 	Loss: 2.2354
[132 / 250], 	Loss: 2.2437
[133 / 250], 	Loss: 2.2374
[134 / 250], 	Loss: 2.2308
[135 / 250], 	Loss: 2.2407
[136 / 250], 	Loss: 2.2406
[137 / 250], 	Loss: 2.2496
[138 / 250], 	Loss: 2.2420
[139 / 250], 	Loss: 2.2322
[140 / 250], 	Loss: 2.2378
[141 / 250], 	Loss: 2.2262
[142 / 250], 	Loss: 2.2354
[143 / 250], 	Loss: 2.2234
[144 / 250], 	Loss: 2.2403
[145 / 250], 	Loss: 2.2375
[146 / 250], 	Loss: 2.2400
[147 / 250], 	Loss: 2.2172
[148 / 250], 	Loss: 2.2428
[149 / 250], 	Loss: 2.2228
[150 / 250], 	Loss: 2.2231
[151 / 250], 	Loss: 2.2436
[152 / 250], 	Loss: 2.2333
[153 / 250], 	Loss: 2.2321
[154 / 250], 	Loss: 2.2377
[155 / 250], 	Loss: 2.2234
[156 / 250], 	Loss: 2.2307
[157 / 250], 	Loss: 2.2379
[158 / 250], 	Loss: 2.2280
[159 / 250], 	Loss: 2.2277
[160 / 250], 	Loss: 2.2310
[161 / 250], 	Loss: 2.2215
[162 / 250], 	Loss: 2.2328
[163 / 250], 	Loss: 2.2302
[164 / 250], 	Loss: 2.2289
[165 / 250], 	Loss: 2.2336
[166 / 250], 	Loss: 2.2297
[167 / 250], 	Loss: 2.2287
[168 / 250], 	Loss: 2.2265
[169 / 250], 	Loss: 2.2201
[170 / 250], 	Loss: 2.2383
[171 / 250], 	Loss: 2.2372
[172 / 250], 	Loss: 2.2323
[173 / 250], 	Loss: 2.2391
[174 / 250], 	Loss: 2.2276
[175 / 250], 	Loss: 2.2276
[176 / 250], 	Loss: 2.2284
[177 / 250], 	Loss: 2.2194
[178 / 250], 	Loss: 2.2257
[179 / 250], 	Loss: 2.2266
[180 / 250], 	Loss: 2.2299
[181 / 250], 	Loss: 2.2371
[182 / 250], 	Loss: 2.2350
[183 / 250], 	Loss: 2.2279
[184 / 250], 	Loss: 2.2387
[185 / 250], 	Loss: 2.2305
[186 / 250], 	Loss: 2.2409
[187 / 250], 	Loss: 2.2413
[188 / 250], 	Loss: 2.2328
[189 / 250], 	Loss: 2.2380
[190 / 250], 	Loss: 2.2305
[191 / 250], 	Loss: 2.2317
[192 / 250], 	Loss: 2.2379
[193 / 250], 	Loss: 2.2274
[194 / 250], 	Loss: 2.2306
[195 / 250], 	Loss: 2.2284
[196 / 250], 	Loss: 2.2297
[197 / 250], 	Loss: 2.2308
[198 / 250], 	Loss: 2.2219
[199 / 250], 	Loss: 2.2254
[200 / 250], 	Loss: 2.2243
[201 / 250], 	Loss: 2.2258
[202 / 250], 	Loss: 2.2234
[203 / 250], 	Loss: 2.2409
[204 / 250], 	Loss: 2.2306
[205 / 250], 	Loss: 2.2290
[206 / 250], 	Loss: 2.2382
[207 / 250], 	Loss: 2.2313
[208 / 250], 	Loss: 2.2247
[209 / 250], 	Loss: 2.2326
[210 / 250], 	Loss: 2.2228
[211 / 250], 	Loss: 2.2217
[212 / 250], 	Loss: 2.2270
[213 / 250], 	Loss: 2.2368
[214 / 250], 	Loss: 2.2156
[215 / 250], 	Loss: 2.2214
[216 / 250], 	Loss: 2.2285
[217 / 250], 	Loss: 2.2212
[218 / 250], 	Loss: 2.2322
[219 / 250], 	Loss: 2.2335
[220 / 250], 	Loss: 2.2351
[221 / 250], 	Loss: 2.2245
[222 / 250], 	Loss: 2.2316
[223 / 250], 	Loss: 2.2223
[224 / 250], 	Loss: 2.2293
[225 / 250], 	Loss: 2.2243
[226 / 250], 	Loss: 2.2369
[227 / 250], 	Loss: 2.2310
[228 / 250], 	Loss: 2.2367
[229 / 250], 	Loss: 2.2371
[230 / 250], 	Loss: 2.2324
[231 / 250], 	Loss: 2.2320
[232 / 250], 	Loss: 2.2231
[233 / 250], 	Loss: 2.2369
[234 / 250], 	Loss: 2.2230
[235 / 250], 	Loss: 2.2286
[236 / 250], 	Loss: 2.2351
[237 / 250], 	Loss: 2.2324
[238 / 250], 	Loss: 2.2230
[239 / 250], 	Loss: 2.2281
[240 / 250], 	Loss: 2.2266
[241 / 250], 	Loss: 2.2366
[242 / 250], 	Loss: 2.2246
[243 / 250], 	Loss: 2.2246
[244 / 250], 	Loss: 2.2314
[245 / 250], 	Loss: 2.2462
[246 / 250], 	Loss: 2.2258
[247 / 250], 	Loss: 2.2298
[248 / 250], 	Loss: 2.2222
[249 / 250], 	Loss: 2.2307
[250 / 250], 	Loss: 2.2306

The accuracy of the 1st task is: 0.1000%
The accuracy of current task 9 is: 77.1000%
The average accuracy is: 24.5778%

------------------------------ Task 9 Training End ------------------------------


------------------------------ Task 10 Training Begin ------------------------------

The number of train set: 5000. The number of test set: 1000

# new classes:  10
[  1 / 250], 	Loss: 3.3596
[  2 / 250], 	Loss: 2.9425
[  3 / 250], 	Loss: 2.8328
[  4 / 250], 	Loss: 2.8123
[  5 / 250], 	Loss: 2.7160
[  6 / 250], 	Loss: 2.6677
[  7 / 250], 	Loss: 2.6384
[  8 / 250], 	Loss: 2.5856
[  9 / 250], 	Loss: 2.5531
[ 10 / 250], 	Loss: 2.5410
[ 11 / 250], 	Loss: 2.5366
[ 12 / 250], 	Loss: 2.4983
[ 13 / 250], 	Loss: 2.4472
[ 14 / 250], 	Loss: 2.4531
[ 15 / 250], 	Loss: 2.4574
[ 16 / 250], 	Loss: 2.3887
[ 17 / 250], 	Loss: 2.3489
[ 18 / 250], 	Loss: 2.3736
[ 19 / 250], 	Loss: 2.3926
[ 20 / 250], 	Loss: 2.3608
[ 21 / 250], 	Loss: 2.3309
[ 22 / 250], 	Loss: 2.3321
[ 23 / 250], 	Loss: 2.3161
[ 24 / 250], 	Loss: 2.3190
[ 25 / 250], 	Loss: 2.3116
[ 26 / 250], 	Loss: 2.3318
[ 27 / 250], 	Loss: 2.2922
[ 28 / 250], 	Loss: 2.2634
[ 29 / 250], 	Loss: 2.3352
[ 30 / 250], 	Loss: 2.3619
[ 31 / 250], 	Loss: 2.3070
[ 32 / 250], 	Loss: 2.2909
[ 33 / 250], 	Loss: 2.2407
[ 34 / 250], 	Loss: 2.2535
[ 35 / 250], 	Loss: 2.2597
[ 36 / 250], 	Loss: 2.2511
[ 37 / 250], 	Loss: 2.2713
[ 38 / 250], 	Loss: 2.2355
[ 39 / 250], 	Loss: 2.2250
[ 40 / 250], 	Loss: 2.2505
[ 41 / 250], 	Loss: 2.2314
[ 42 / 250], 	Loss: 2.2471
[ 43 / 250], 	Loss: 2.2493
[ 44 / 250], 	Loss: 2.2735
[ 45 / 250], 	Loss: 2.2420
[ 46 / 250], 	Loss: 2.2312
[ 47 / 250], 	Loss: 2.2124
[ 48 / 250], 	Loss: 2.1971
[ 49 / 250], 	Loss: 2.2074
[ 50 / 250], 	Loss: 2.2088
[ 51 / 250], 	Loss: 2.2404
[ 52 / 250], 	Loss: 2.2382
[ 53 / 250], 	Loss: 2.2228
[ 54 / 250], 	Loss: 2.2202
[ 55 / 250], 	Loss: 2.2171
[ 56 / 250], 	Loss: 2.2669
[ 57 / 250], 	Loss: 2.3500
[ 58 / 250], 	Loss: 2.2854
[ 59 / 250], 	Loss: 2.3362
[ 60 / 250], 	Loss: 2.2579
[ 61 / 250], 	Loss: 2.2097
[ 62 / 250], 	Loss: 2.2209
[ 63 / 250], 	Loss: 2.2602
[ 64 / 250], 	Loss: 2.2464
[ 65 / 250], 	Loss: 2.2071
[ 66 / 250], 	Loss: 2.2373
[ 67 / 250], 	Loss: 2.2049
[ 68 / 250], 	Loss: 2.2717
[ 69 / 250], 	Loss: 2.3133
[ 70 / 250], 	Loss: 2.3316
[ 71 / 250], 	Loss: 2.3059
[ 72 / 250], 	Loss: 2.2319
[ 73 / 250], 	Loss: 2.2069
[ 74 / 250], 	Loss: 2.2008
[ 75 / 250], 	Loss: 2.2041
[ 76 / 250], 	Loss: 2.2649
[ 77 / 250], 	Loss: 2.2550
[ 78 / 250], 	Loss: 2.2225
[ 79 / 250], 	Loss: 2.2003
[ 80 / 250], 	Loss: 2.2101
[ 81 / 250], 	Loss: 2.1777
[ 82 / 250], 	Loss: 2.1761
[ 83 / 250], 	Loss: 2.1843
[ 84 / 250], 	Loss: 2.1655
[ 85 / 250], 	Loss: 2.2410
[ 86 / 250], 	Loss: 2.2819
[ 87 / 250], 	Loss: 2.2056
[ 88 / 250], 	Loss: 2.1805
[ 89 / 250], 	Loss: 2.1904
[ 90 / 250], 	Loss: 2.2057
[ 91 / 250], 	Loss: 2.1772
[ 92 / 250], 	Loss: 2.1883
[ 93 / 250], 	Loss: 2.1601
[ 94 / 250], 	Loss: 2.1660
[ 95 / 250], 	Loss: 2.1823
[ 96 / 250], 	Loss: 2.1822
[ 97 / 250], 	Loss: 2.1878
[ 98 / 250], 	Loss: 2.1731
[ 99 / 250], 	Loss: 2.1671
[100 / 250], 	Loss: 2.1764
[101 / 250], 	Loss: 2.1679
[102 / 250], 	Loss: 2.1442
[103 / 250], 	Loss: 2.1452
[104 / 250], 	Loss: 2.1431
[105 / 250], 	Loss: 2.1537
[106 / 250], 	Loss: 2.1414
[107 / 250], 	Loss: 2.1357
[108 / 250], 	Loss: 2.1394
[109 / 250], 	Loss: 2.1510
[110 / 250], 	Loss: 2.1418
[111 / 250], 	Loss: 2.1318
[112 / 250], 	Loss: 2.1353
[113 / 250], 	Loss: 2.1290
[114 / 250], 	Loss: 2.1394
[115 / 250], 	Loss: 2.1346
[116 / 250], 	Loss: 2.1419
[117 / 250], 	Loss: 2.1535
[118 / 250], 	Loss: 2.1481
[119 / 250], 	Loss: 2.1360
[120 / 250], 	Loss: 2.1283
[121 / 250], 	Loss: 2.1451
[122 / 250], 	Loss: 2.1316
[123 / 250], 	Loss: 2.1380
[124 / 250], 	Loss: 2.1413
[125 / 250], 	Loss: 2.1288
[126 / 250], 	Loss: 2.1400
[127 / 250], 	Loss: 2.1307
[128 / 250], 	Loss: 2.1384
[129 / 250], 	Loss: 2.1338
[130 / 250], 	Loss: 2.1342
[131 / 250], 	Loss: 2.1392
[132 / 250], 	Loss: 2.1235
[133 / 250], 	Loss: 2.1295
[134 / 250], 	Loss: 2.1163
[135 / 250], 	Loss: 2.1243
[136 / 250], 	Loss: 2.1280
[137 / 250], 	Loss: 2.1326
[138 / 250], 	Loss: 2.1414
[139 / 250], 	Loss: 2.1331
[140 / 250], 	Loss: 2.1232
[141 / 250], 	Loss: 2.1237
[142 / 250], 	Loss: 2.1422
[143 / 250], 	Loss: 2.1242
[144 / 250], 	Loss: 2.1311
[145 / 250], 	Loss: 2.1300
[146 / 250], 	Loss: 2.1446
[147 / 250], 	Loss: 2.1413
[148 / 250], 	Loss: 2.1351
[149 / 250], 	Loss: 2.1228
[150 / 250], 	Loss: 2.1212
[151 / 250], 	Loss: 2.1175
[152 / 250], 	Loss: 2.1225
[153 / 250], 	Loss: 2.1338
[154 / 250], 	Loss: 2.1228
[155 / 250], 	Loss: 2.1106
[156 / 250], 	Loss: 2.1435
[157 / 250], 	Loss: 2.1241
[158 / 250], 	Loss: 2.1143
[159 / 250], 	Loss: 2.1214
[160 / 250], 	Loss: 2.1298
[161 / 250], 	Loss: 2.1347
[162 / 250], 	Loss: 2.1254
[163 / 250], 	Loss: 2.1352
[164 / 250], 	Loss: 2.1232
[165 / 250], 	Loss: 2.1187
[166 / 250], 	Loss: 2.1182
[167 / 250], 	Loss: 2.1348
[168 / 250], 	Loss: 2.1263
[169 / 250], 	Loss: 2.1251
[170 / 250], 	Loss: 2.1188
[171 / 250], 	Loss: 2.1246
[172 / 250], 	Loss: 2.1363
[173 / 250], 	Loss: 2.1267
[174 / 250], 	Loss: 2.1147
[175 / 250], 	Loss: 2.1184
[176 / 250], 	Loss: 2.1151
[177 / 250], 	Loss: 2.1191
[178 / 250], 	Loss: 2.1232
[179 / 250], 	Loss: 2.1147
[180 / 250], 	Loss: 2.1333
[181 / 250], 	Loss: 2.1232
[182 / 250], 	Loss: 2.1211
[183 / 250], 	Loss: 2.1161
[184 / 250], 	Loss: 2.1296
[185 / 250], 	Loss: 2.1270
[186 / 250], 	Loss: 2.1267
[187 / 250], 	Loss: 2.1313
[188 / 250], 	Loss: 2.1230
[189 / 250], 	Loss: 2.1288
[190 / 250], 	Loss: 2.1350
[191 / 250], 	Loss: 2.1394
[192 / 250], 	Loss: 2.1204
[193 / 250], 	Loss: 2.1268
[194 / 250], 	Loss: 2.1183
[195 / 250], 	Loss: 2.1168
[196 / 250], 	Loss: 2.1172
[197 / 250], 	Loss: 2.1292
[198 / 250], 	Loss: 2.1256
[199 / 250], 	Loss: 2.1287
[200 / 250], 	Loss: 2.1164
[201 / 250], 	Loss: 2.1226
[202 / 250], 	Loss: 2.1079
[203 / 250], 	Loss: 2.1235
[204 / 250], 	Loss: 2.1174
[205 / 250], 	Loss: 2.1236
[206 / 250], 	Loss: 2.1265
[207 / 250], 	Loss: 2.1284
[208 / 250], 	Loss: 2.1270
[209 / 250], 	Loss: 2.1315
[210 / 250], 	Loss: 2.1282
[211 / 250], 	Loss: 2.1180
[212 / 250], 	Loss: 2.1215
[213 / 250], 	Loss: 2.1287
[214 / 250], 	Loss: 2.1224
[215 / 250], 	Loss: 2.1281
[216 / 250], 	Loss: 2.1331
[217 / 250], 	Loss: 2.1247
[218 / 250], 	Loss: 2.1328
[219 / 250], 	Loss: 2.1226
[220 / 250], 	Loss: 2.1354
[221 / 250], 	Loss: 2.1283
[222 / 250], 	Loss: 2.1260
[223 / 250], 	Loss: 2.1330
[224 / 250], 	Loss: 2.1357
[225 / 250], 	Loss: 2.1255
[226 / 250], 	Loss: 2.1284
[227 / 250], 	Loss: 2.1368
[228 / 250], 	Loss: 2.1345
[229 / 250], 	Loss: 2.1270
[230 / 250], 	Loss: 2.1443
[231 / 250], 	Loss: 2.1219
[232 / 250], 	Loss: 2.1405
[233 / 250], 	Loss: 2.1370
[234 / 250], 	Loss: 2.1118
[235 / 250], 	Loss: 2.1235
[236 / 250], 	Loss: 2.1370
[237 / 250], 	Loss: 2.1212
[238 / 250], 	Loss: 2.1274
[239 / 250], 	Loss: 2.1334
[240 / 250], 	Loss: 2.1262
[241 / 250], 	Loss: 2.1172
[242 / 250], 	Loss: 2.1074
[243 / 250], 	Loss: 2.1188
[244 / 250], 	Loss: 2.1309
[245 / 250], 	Loss: 2.1192
[246 / 250], 	Loss: 2.1313
[247 / 250], 	Loss: 2.1217
[248 / 250], 	Loss: 2.1183
[249 / 250], 	Loss: 2.1266
[250 / 250], 	Loss: 2.1244

The accuracy of the 1st task is: 0.0000%
The accuracy of current task 10 is: 65.1000%
The average accuracy is: 23.4900%

------------------------------ Task 10 Training End ------------------------------


